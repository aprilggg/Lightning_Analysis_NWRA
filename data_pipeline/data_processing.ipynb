{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data - Creating and Cleaning Consolidated Data Files\n",
    "\n",
    "This notebook includes the following sections:\n",
    "- Combining Files\n",
    "- Cleaning and Filtering\n",
    "- Binning the Lightning Data\n",
    "- Joining the Datasets\n",
    "- Calculating Current Category and Intensification Change\n",
    "\n",
    "This notebook should be executed after the `data_file_cleaning.ipynb` notebook. \n",
    "\n",
    "In the Combining Files section, we use the Google Drive API to grab and combine all the separate files in the folder where our data lives. We also parse the tropical cyclone (TC) ID and name from each of the file names to add as columns. This section will create intermediate files in a directory called `processed_files`, and `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt` in a directory called `combined_files` for use in the next section. The `processed_files` directory and its contents can be deleted after completion of this section.\n",
    "\n",
    "In the Cleaning and Filtering section, we perform some post-processing on the data by adding column headers, filtering to tropical cyclones that are category 1 or higher, and calculating the direct distance of each lightning strike to the TC storm center. This will create additional `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files. \n",
    "\n",
    "The Binning the Lightning Data section creates 30 minute bins for use in evaluating lightning burst thresholds and comparison with TC wind and pressure data. This section yields `WWLLN_innercore.csv`, `WWLLN_rainband.csv`, `WWLLN_innercore_w_time.csv`, `WWLLN_innercore_timebin_count.csv`,  `WWLLN_rainband_timebin_count.csv`, `WWLLN_rainband_w_time.csv`.\n",
    "\n",
    "In Joining the Datasets, we join the WWLLN datasets with the trackfile dataset. We join each 30-minute bin with the nearest wind and pressure data. This section does not create any files.\n",
    "\n",
    "The last part of this notebook, Calculating Current Category and Intensification Change, calculates category and intensification change bins for use in the burst threshold analysis. This portion will create the `innercore_joined.csv`, `innercore_joined_w_time.csv`, `rainband_joined.csv`, `rainband_joined_w_time.csv` files for use in analysis.\n",
    "\n",
    "This notebook outputs the following files:\n",
    "- `combined_files/Combined_Reduced_Trackfile.txt`\n",
    "- `combined_files/Combined_WWLLN_Locations.txt`\n",
    "- `Filtered_Reduced_Trackfile.csv`\n",
    "- `Filtered_WWLLN_Locations.txt`\n",
    "- `WWLLN_innercore.csv`\n",
    "- `WWLLN_rainband.csv`\n",
    "- `WWLLN_innercore_w_time.csv`\n",
    "- `WWLLN_innercore_timebin_count.csv`\n",
    "- `WWLLN_rainband_timebin_count.csv`\n",
    "- `WWLLN_rainband_w_time.csv`\n",
    "- `innercore_joined.csv`*\n",
    "- `innercore_joined_w_time.csv`\n",
    "- `rainband_joined.csv`*\n",
    "- `rainband_joined_w_time.csv`\n",
    "\n",
    "*denotes files used in the subsequent lightning burst threshold analysis.\n",
    "\n",
    "### Combining Files\n",
    "We use the [Google Drive API](https://developers.google.com/drive/api/guides/about-sdk) to download the files previously uploaded in `data_upload.ipynb` to consolidate the individual files. The first half of the code works if the Google Drive API is already set up (refer to instructions in `data_upload.ipynb`). The code after we create the list of files is not dependent on the Google Drive API. This section will create intermediate files in a directory called `processed_files`, and `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt` in a directory called `combined_files` for use in the next section. The `processed_files` directory and its contents can be deleted after completion of this section.\n",
    "\n",
    "Let's start by installing necessary packages and then importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import os\n",
    "import polars as pl\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from io import BytesIO\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the function in `data_file_cleaning.ipynb`, we use the following function to authenticate the Google Drive API. This will open a browser to perform the authentication process. \n",
    "\n",
    "Check if a `token.pickle` file already exists before running the following code. If the file exists, it is recommended to delete it before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=389849867563-4uggnm57nqe52156v32gj1lkosoqpoem.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A37635%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=GyfvJGpr2mtpuRzI6AP8xi3h1k3QIH&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# Scopes for accessing Google Drive\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Authenticate and create the service object\n",
    "def authenticate_drive_api():\n",
    "    creds = None\n",
    "    # Token file for saving the authentication\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no credentials, perform authentication\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)  # Ensure 'credentials.json' is downloaded from Google API Console\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for future use\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Initialize the service object\n",
    "service = authenticate_drive_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function grabs the list of all files in a specified folder that are not trashed and stores them into a list. Each file has an ID and name attribute that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(folder_id):\n",
    "    # Query to find files in the specified folder\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    files = []\n",
    "\n",
    "    # List files in the folder and append to list\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields='nextPageToken, files(id, name)',\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        files += response.get('files', [])\n",
    "\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to find files in the specified folder. The folder ID can be found as the string after the \"folders/\" part of the URL for the Google Drive folder. This will give us a list of files to iterate through for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the folder\n",
    "folder_id = '14idmMBbM5xXZg4b61iINHbBTl2z4yLeD'\n",
    "files = find_files(folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split out the tropical cyclone ID and name from each of the files to add as a separate column. We then save the files in the `processed_files` directory for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file to add cyclone ID and name as columns\n",
    "# Directory to save the processed files locally\n",
    "output_dir = \"processed_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "\n",
    "    # Extract the cyclone ID and name from the filename\n",
    "    cyclone_id = '_'.join(file_name.split('_')[:3])\n",
    "    cyclone_name = file_name.split('_')[3]\n",
    "\n",
    "    # Download the file content\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_stream = BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_stream, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "    file_stream.seek(0)\n",
    "    content = file_stream.read().decode('utf-8')\n",
    "\n",
    "    # Add the cyclone id and name as a new column using Polars\n",
    "    df = pl.read_csv(BytesIO(content.encode('utf-8')),separator='\\t', has_header=False)\n",
    "    df = df.with_columns([\n",
    "    pl.lit(cyclone_id).alias(\"cyclone_id\"),\n",
    "    pl.lit(cyclone_name).alias(\"cyclone_name\")\n",
    "    ])\n",
    "\n",
    "    # Save the modified DataFrame locally\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    df.write_csv(output_file_path, separator='\\t',include_header=False)\n",
    "\n",
    "    print(f\"Processed and saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine each of the trackfiles in the `processed_files` folder into one file, and each of the WWLLN location files into one file. This will give us two output files in the `combined_files` folder - `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt`. We will use these files as the basis for the next portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining 992 files for pattern 'Reduced_Trackfile'...\n",
      "Combined file saved: combined_files/Combined_Reduced_Trackfile.txt\n",
      "Combining 994 files for pattern 'WWLLN_Locations'...\n",
      "Combined file saved: combined_files/Combined_WWLLN_Locations.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Directories for processed files and output\n",
    "input_dir = \"processed_files\"\n",
    "output_dir = \"combined_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File patterns to combine\n",
    "patterns = {\n",
    "    \"Reduced_Trackfile\": os.path.join(input_dir, \"*Reduced_Trackfile*.txt\"),\n",
    "    \"WWLLN_Locations\": os.path.join(input_dir, \"*WWLLN_Locations*.txt\")\n",
    "}\n",
    "\n",
    "# Combine files based on patterns\n",
    "for pattern_name, pattern_path in patterns.items():\n",
    "    combined_content = []\n",
    "    output_file_path = os.path.join(output_dir, f\"Combined_{pattern_name}.txt\")\n",
    "\n",
    "    # Find all matching files\n",
    "    matching_files = glob.glob(pattern_path)\n",
    "    print(f\"Combining {len(matching_files)} files for pattern '{pattern_name}'...\")\n",
    "\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for file_path in matching_files:\n",
    "            with open(file_path, \"r\") as input_file:\n",
    "                for line in input_file:\n",
    "                    output_file.write(line)\n",
    "\n",
    "    print(f\"Combined file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Filtering\n",
    "In this section we add a column header to the files and filter down to TCs that are category 1 and above. Category 1 is defined using the [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php), where the maximum sustained wind speed is between 64-82 kt. We calculate each TC's category using the Saffir-Simpson Scale and save it in a new column. We then calculate the direct distance of each lightning strike to the storm center and denote it as inner core or rainband. This section outputs `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files.\n",
    "\n",
    "Start by importing the necessary libraries and files created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trackfile and WWLLN file from google drive\n",
    "track_file = pd.read_csv(\"Combined_Reduced_Trackfile.txt\", sep=\"\\t\")\n",
    "track_file = track_file.drop(track_file.columns[8], axis=1)\n",
    "\n",
    "# Use chuck due to the large file size\n",
    "chunksize = 100000  \n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    \"Combined_WWLLN_Locations.txt\",\n",
    "    delim_whitespace=True,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add headers to the two dataframes for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file.columns = ['year', 'month', 'day','hour','lat','lon','pressure', 'knots', 'storm_code', 'storm_name']\n",
    "locations_WWLLN.columns = ['year', 'month', 'day', 'hour', 'min', 'sec','lat','lon','distance_from_storm_center_km_east', 'distance_from_storm_center_km_north', 'storm_code','storm_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the trackfile data by creating the list of storm codes that meet the category 1 or higher requirement. We will use this list to filter the wind speed/pressure data as well as the lightning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>max_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL_10_10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  max_wind_speed\n",
       "0   ATL_10_1              85\n",
       "1  ATL_10_10              30\n",
       "2  ATL_10_11             135\n",
       "3  ATL_10_12             120\n",
       "4  ATL_10_13             105"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the max wind speed for each storm code\n",
    "\n",
    "max_wind_speed = track_file.groupby('storm_code').agg(\n",
    "    max_wind_speed=('knots', 'max')\n",
    ").reset_index()\n",
    "max_wind_speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ATL_10_14</td>\n",
       "      <td>1</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  category basin\n",
       "0   ATL_10_1         2   ATL\n",
       "2  ATL_10_11         4   ATL\n",
       "3  ATL_10_12         4   ATL\n",
       "4  ATL_10_13         3   ATL\n",
       "5  ATL_10_14         1   ATL"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by max >= 64 knots\n",
    "storm_filter = max_wind_speed[max_wind_speed[\"max_wind_speed\"] >= 64].copy()\n",
    "\n",
    "# calculate the TC category using the max wind speed\n",
    "storm_filter[\"category\"] = storm_filter[\"max_wind_speed\"].apply(\n",
    "    lambda x: 1 if 64 <= x <= 82 else (2 if 82 < x <= 95 else (3 if 95 < x <= 112 else (4 if 112 < x <= 136 else (5 if x > 136 else 0))))\n",
    ")\n",
    "storm_filter = storm_filter[[\"storm_code\", \"category\"]]\n",
    "\n",
    "# strip the basin from the storm code\n",
    "storm_filter[\"basin\"] = storm_filter[\"storm_code\"].str.extract(r\"^(.*?)_\")\n",
    "storm_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of TCs: 982, category 1 or higher number of TCs: 473\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall number of TCs: {len(max_wind_speed)}, category 1 or higher number of TCs: {len(storm_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  \n",
       "0         2   ATL  \n",
       "1         2   ATL  \n",
       "2         2   ATL  \n",
       "3         2   ATL  \n",
       "4         2   ATL  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the trackfile data by the storm filter\n",
    "track_file_filtered = track_file[track_file[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "# join the category column by storm code\n",
    "track_file_filtered = pd.merge(track_file_filtered, storm_filter, how='inner', on='storm_code')\n",
    "\n",
    "track_file_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a csv file for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file_filtered.to_csv('Filtered_Reduced_Trackfile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on the WWLLN dataset. Start by filtering the WWLLN dataset by the storm filter created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter WWLLN dataset by the storm filter\n",
    "locations_WWLLN_filtered = locations_WWLLN[locations_WWLLN[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "locations_WWLLN_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the direct distance of each lightning instance from the storm center using a simple triangle calculation. We have the north and east distances from center, so we use the Pythagorean theorem to simply calculate the missing hypotenuse. We also create an indicator for inner core lightning and another for rainband lightning. Inner core lightning is defined as within 100km of storm center, while rainband lightning is defined as between 200-400km of storm center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered['hypotenuse_disance_from_storm_center'] = np.sqrt(locations_WWLLN_filtered['distance_from_storm_center_km_east'] ** 2 +locations_WWLLN['distance_from_storm_center_km_north'] ** 2)\n",
    "locations_WWLLN_filtered[\"inner_core_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if x <= 100 else 0\n",
    ")\n",
    "locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if (x >= 200 and x <= 400) else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a txt file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered.to_csv(\"Filtered_WWLLN_Locations.txt\", sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning the Lightning Data\n",
    "To compare the lightning bursts with TC wind and pressure data, we must bin and join the trackfile and WWLLN datasets. Because of the size of the data we're working with, we can restart the kernel here to keep memory use more optimal. \n",
    "\n",
    "We will first bin the lightning data into 30-minute bins, yielding 48 bins a day. Each bin contains a count of lightning events during that timeframe. We will then join the binned data to the trackfile data.\n",
    "\n",
    "Let's start by importing necessary libraries and files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the WWLLN file from earlier, but filtered to keep either rainband or innercore\n",
    "\n",
    "chunksize = 100000  \n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    r\"C:\\Users\\user\\Desktop\\25 WI\\Filtered_WWLLN_Locations.txt\",\n",
    "    delim_whitespace=True,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN_filtered_ = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial analysis, we are only interested in either inner core or rainband lighting. We drop all other lightning events that are not categorized as either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the inner core or rainband lightning events using the indicator columns\n",
    "locations_WWLLN_filtered_inner_rainband = locations_WWLLN_filtered_[\n",
    "    ~((locations_WWLLN_filtered_[\"rainband_ind\"] == 0) & (locations_WWLLN_filtered_[\"inner_core_ind\"] == 0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out just the inner core data\n",
    "locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_inner_rainband[locations_WWLLN_filtered_inner_rainband['inner_core_ind'] == 1]\n",
    "# print(locations_WWLLN_filtered_innercore.shape)\n",
    "# (2611951, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out just the rainband data\n",
    "locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_inner_rainband[locations_WWLLN_filtered_inner_rainband['rainband_ind'] == 1]\n",
    "# print(locations_WWLLN_filtered_rainband.shape)\n",
    "# (10146702, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the inner core and rainband datasets as separate csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore.csv\", index=False)\n",
    "locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restart the kernel again here to free up memory. Next, we will create 30-minute bins for both the inner core and rainband datasets.\n",
    "\n",
    "**!!! need to fix missing bins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the files we just exported\n",
    "locations_WWLLN_filtered_innercore = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore.csv\")\n",
    "locations_WWLLN_filtered_rainband = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prev Version, keep this for record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8260\\4028445333.py:18: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>time_bin</th>\n",
       "      <th>lightning_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-21 07:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 14:30:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 15:00:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 16:30:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 22:30:00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58145</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 12:30:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58146</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 13:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58147</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 14:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58148</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 14:30:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58149</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 15:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58150 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      storm_code            time_bin  lightning_count\n",
       "0       ATL_10_1 2010-06-21 07:00:00                1\n",
       "1       ATL_10_1 2010-06-23 14:30:00                1\n",
       "2       ATL_10_1 2010-06-23 15:00:00                3\n",
       "3       ATL_10_1 2010-06-23 16:30:00                2\n",
       "4       ATL_10_1 2010-06-23 22:30:00                4\n",
       "...          ...                 ...              ...\n",
       "58145  WPAC_20_9 2020-08-25 12:30:00                2\n",
       "58146  WPAC_20_9 2020-08-25 13:00:00                2\n",
       "58147  WPAC_20_9 2020-08-25 14:00:00                2\n",
       "58148  WPAC_20_9 2020-08-25 14:30:00                1\n",
       "58149  WPAC_20_9 2020-08-25 15:00:00                1\n",
       "\n",
       "[58150 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # bin the inner core data\n",
    "# # Create a datetime column from the existing columns\n",
    "# locations_WWLLN_filtered_innercore['sec'] = locations_WWLLN_filtered_innercore['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# locations_WWLLN_filtered_innercore['datetime'] = pd.to_datetime(locations_WWLLN_filtered_innercore['year'].astype(str) + '-' +\n",
    "#                                  locations_WWLLN_filtered_innercore['month'].astype(str).str.zfill(2) + '-' +\n",
    "#                                  locations_WWLLN_filtered_innercore['day'].astype(str).str.zfill(2) + ' ' +\n",
    "#                                  locations_WWLLN_filtered_innercore['hour'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_innercore['min'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_innercore['sec'].astype(str).str.zfill(2))\n",
    "\n",
    "# # Define a function to apply the 30-minute binning for each storm_code group\n",
    "# def add_time_bin(group):\n",
    "#     group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "#     return group\n",
    "\n",
    "# # Group by storm_code and apply the binning function\n",
    "# locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# # Print the resulting DataFrame with the new 'time_bin' column\n",
    "# locations_WWLLN_filtered_innercore.head()\n",
    "\n",
    "# # group by bins and get the count per 30-minute bin\n",
    "# locations_WWLLN_filtered_innercore_grouped = locations_WWLLN_filtered_innercore.groupby(['storm_code', 'time_bin'])\n",
    "# locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(by = ['storm_code', 'time_bin'])\n",
    "# locations_WWLLN_filtered_innercore_timebin\n",
    "# # export both the ungrouped and grouped datasets to csv files\n",
    "# # locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore_w_time.csv\", index=False)\n",
    "# # locations_WWLLN_filtered_innercore_timebin.to_csv(\"WWLLN_innercore_timebin_count.csv\", index=False)\n",
    "\n",
    "\n",
    "#### Rain Band\n",
    "\n",
    "# # bin the rainband data\n",
    "# # Create a datetime column from the existing columns\n",
    "# locations_WWLLN_filtered_rainband['sec'] = locations_WWLLN_filtered_rainband['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# locations_WWLLN_filtered_rainband['datetime'] = pd.to_datetime(locations_WWLLN_filtered_rainband['year'].astype(str) + '-' +\n",
    "#                                  locations_WWLLN_filtered_rainband['month'].astype(str).str.zfill(2) + '-' +\n",
    "#                                  locations_WWLLN_filtered_rainband['day'].astype(str).str.zfill(2) + ' ' +\n",
    "#                                  locations_WWLLN_filtered_rainband['hour'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_rainband['min'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_rainband['sec'].astype(str).str.zfill(2))\n",
    "\n",
    "# # Define a function to apply the 30-minute binning for each storm_code group\n",
    "# def add_time_bin(group):\n",
    "#     group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "#     return group\n",
    "\n",
    "# # Group by storm_code and apply the binning function\n",
    "# locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# # Print the resulting DataFrame with the new 'time_bin' column\n",
    "# locations_WWLLN_filtered_rainband.head()\n",
    "# # group by bins and get the count per 30-minute bin\n",
    "# locations_WWLLN_filtered_rainband_grouped = locations_WWLLN_filtered_rainband.groupby(['storm_code', 'time_bin'])\n",
    "# locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.sort_values(by = ['storm_code', 'time_bin'])\n",
    "\n",
    "# # export both the ungrouped and grouped datasets to csv files\n",
    "# locations_WWLLN_filtered_rainband_timebin.to_csv(\"WWLLN_rainband_timebin_count.csv\", index=False)\n",
    "# locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband_w_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8260\\1105446163.py:20: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n"
     ]
    }
   ],
   "source": [
    "# Ensure sec column is valid\n",
    "locations_WWLLN_filtered_innercore['sec'] = locations_WWLLN_filtered_innercore['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# Create a datetime column\n",
    "locations_WWLLN_filtered_innercore['datetime'] = pd.to_datetime(\n",
    "    locations_WWLLN_filtered_innercore['year'].astype(str) + '-' +\n",
    "    locations_WWLLN_filtered_innercore['month'].astype(str).str.zfill(2) + '-' +\n",
    "    locations_WWLLN_filtered_innercore['day'].astype(str).str.zfill(2) + ' ' +\n",
    "    locations_WWLLN_filtered_innercore['hour'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_innercore['min'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_innercore['sec'].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "# Define a function to apply the 30-minute binning for each storm_code group\n",
    "def add_time_bin(group):\n",
    "    group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "    return group\n",
    "\n",
    "# Group by storm_code and apply the binning function\n",
    "locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# Group by bins and get the count per 30-minute bin\n",
    "locations_WWLLN_filtered_innercore_grouped = locations_WWLLN_filtered_innercore.groupby(['storm_code', 'time_bin'])\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# Add missing 30-minute bins for each storm_code\n",
    "def add_missing_bins(group):\n",
    "    min_time, max_time = group['time_bin'].min(), group['time_bin'].max()\n",
    "    \n",
    "    # Create a full range of 30-minute bins for the time period of this storm\n",
    "    full_bins = pd.DataFrame({'time_bin': pd.date_range(min_time, max_time, freq='30T')})\n",
    "    \n",
    "    # Merge the full_bins with the original group\n",
    "    merged = full_bins.merge(group[['storm_code', 'time_bin', 'lightning_count']], how='left', on='time_bin')\n",
    "    \n",
    "    # Fill missing lightning_count with 0 where there is no data\n",
    "    merged['lightning_count'] = merged['lightning_count'].fillna(0).astype(int)\n",
    "    \n",
    "    # Fill missing storm_code with the first valid entry in the group\n",
    "    merged['storm_code'] = merged['storm_code'].fillna(method='ffill')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Apply the function to each storm_code\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.groupby('storm_code', group_keys=False).apply(add_missing_bins)\n",
    "\n",
    "# Sort the final result\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Print the resulting DataFrame with the new 'time_bin' and 'lightning_count' columns\n",
    "locations_WWLLN_filtered_innercore_timebin.head()\n",
    "\n",
    "# # Export both the ungrouped and grouped datasets\n",
    "locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore_w_time.csv\", index=False)\n",
    "locations_WWLLN_filtered_innercore_timebin.to_csv(\"WWLLN_innercore_timebin_count.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8260\\2887306073.py:20: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n"
     ]
    }
   ],
   "source": [
    "# Ensure sec column is valid\n",
    "locations_WWLLN_filtered_rainband['sec'] = locations_WWLLN_filtered_rainband['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# Create a datetime column\n",
    "locations_WWLLN_filtered_rainband['datetime'] = pd.to_datetime(\n",
    "    locations_WWLLN_filtered_rainband['year'].astype(str) + '-' +\n",
    "    locations_WWLLN_filtered_rainband['month'].astype(str).str.zfill(2) + '-' +\n",
    "    locations_WWLLN_filtered_rainband['day'].astype(str).str.zfill(2) + ' ' +\n",
    "    locations_WWLLN_filtered_rainband['hour'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_rainband['min'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_rainband['sec'].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "# Define a function to apply the 30-minute binning for each storm_code group\n",
    "def add_time_bin(group):\n",
    "    group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "    return group\n",
    "\n",
    "# Group by storm_code and apply the binning function\n",
    "locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# Group by bins and get the count per 30-minute bin\n",
    "locations_WWLLN_filtered_rainband_grouped = locations_WWLLN_filtered_rainband.groupby(['storm_code', 'time_bin'])\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# Add missing 30-minute bins for each storm_code\n",
    "def add_missing_bins(group):\n",
    "    min_time, max_time = group['time_bin'].min(), group['time_bin'].max()\n",
    "    \n",
    "    # Create a full range of 30-minute bins for the time period of this storm\n",
    "    full_bins = pd.DataFrame({'time_bin': pd.date_range(min_time, max_time, freq='30T')})\n",
    "    \n",
    "    # Merge the full_bins with the original group\n",
    "    merged = full_bins.merge(group[['storm_code', 'time_bin', 'lightning_count']], how='left', on='time_bin')\n",
    "    \n",
    "    # Fill missing lightning_count with 0 where there is no data\n",
    "    merged['lightning_count'] = merged['lightning_count'].fillna(0).astype(int)\n",
    "    \n",
    "    # Fill missing storm_code with the first valid entry in the group\n",
    "    merged['storm_code'] = merged['storm_code'].fillna(method='ffill')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Apply the function to each storm_code\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.groupby('storm_code', group_keys=False).apply(add_missing_bins)\n",
    "\n",
    "# Sort the final result\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Print the resulting DataFrame with the new 'time_bin' and 'lightning_count' columns\n",
    "locations_WWLLN_filtered_rainband_timebin.head()\n",
    "\n",
    "# # Export both the ungrouped and grouped datasets\n",
    "locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband_w_time.csv\", index=False)\n",
    "locations_WWLLN_filtered_rainband_timebin.to_csv(\"WWLLN_rainband_timebin_count.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the Datasets\n",
    "The trackfile data includes wind and pressure data at regular intervals for each TC. Since there is no function that approximates the behavior of TC wind and pressure change, we will not interpolate the data for timestamps between the intervals - rather, we will join each lightning event to the nearest wind and pressure data and perform our analysis as such. We recognize that this process is a source of error, but we believe that joining to the nearest timestamp will keep this known error as low as possible for the sake of this analysis.\n",
    "\n",
    "Restart the kernel again here to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the necessary files\n",
    "filtered_reduced_trackfile = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\Filtered_Reduced_Trackfile.csv\")\n",
    "locations_WWLLN_filtered_rainband_timebin = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband_timebin_count_updated.csv\")\n",
    "locations_WWLLN_filtered_rainband = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband_w_time_updated.csv\")\n",
    "locations_WWLLN_filtered_innercore_timebin = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore_timebin_count_updated.csv\")\n",
    "locations_WWLLN_filtered_innercore = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore_w_time_updated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the datetime columns into datetime data types to ensure that dates are represented correctly and yield an accurate join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime columns to the correct data type for inner core data\n",
    "locations_WWLLN_filtered_innercore_timebin[\"time_bin\"] = pd.to_datetime(locations_WWLLN_filtered_innercore_timebin[\"time_bin\"])\n",
    "\n",
    "locations_WWLLN_filtered_innercore_timebin[\"year\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.year\n",
    "locations_WWLLN_filtered_innercore_timebin[\"month\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.month\n",
    "locations_WWLLN_filtered_innercore_timebin[\"day\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.day\n",
    "locations_WWLLN_filtered_innercore_timebin[\"hour\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.hour\n",
    "locations_WWLLN_filtered_innercore_timebin[\"minute\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime columns to the correct data type for rainband data\n",
    "locations_WWLLN_filtered_rainband_timebin[\"time_bin\"] = pd.to_datetime(locations_WWLLN_filtered_rainband_timebin[\"time_bin\"])\n",
    "\n",
    "locations_WWLLN_filtered_rainband_timebin[\"year\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.year\n",
    "locations_WWLLN_filtered_rainband_timebin[\"month\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.month\n",
    "locations_WWLLN_filtered_rainband_timebin[\"day\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.day\n",
    "locations_WWLLN_filtered_rainband_timebin[\"hour\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.hour\n",
    "locations_WWLLN_filtered_rainband_timebin[\"minute\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the minute columns\n",
    "locations_WWLLN_filtered_rainband.rename(columns={'min': 'minute'}, inplace=True)\n",
    "locations_WWLLN_filtered_innercore.rename(columns={'min': 'minute'}, inplace=True)\n",
    "filtered_reduced_trackfile['minute'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  minute  \n",
       "0         2   ATL       0  \n",
       "1         2   ATL       0  \n",
       "2         2   ATL       0  \n",
       "3         2   ATL       0  \n",
       "4         2   ATL       0  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_reduced_trackfile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(\n",
    "    [\"storm_code\", \"year\", \"month\", \"day\", \"hour\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "filtered_reduced_trackfile = filtered_reduced_trackfile.sort_values(\n",
    "    [\"storm_code\", \"year\", \"month\", \"day\", \"hour\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to polars for faster processing\n",
    "rainband_count_pl = pl.from_pandas(locations_WWLLN_filtered_rainband_timebin)\n",
    "rainband_w_time_pl = pl.from_pandas(locations_WWLLN_filtered_rainband)\n",
    "rainband_w_time_pl = rainband_w_time_pl.with_columns(\n",
    "    pl.col(\"time_bin\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "\n",
    "\n",
    "innercore_count_pl = pl.from_pandas(locations_WWLLN_filtered_innercore_timebin)\n",
    "innercore_w_time_pl = pl.from_pandas(locations_WWLLN_filtered_innercore)\n",
    "innercore_w_time_pl = innercore_w_time_pl.with_columns(\n",
    "    pl.col(\"time_bin\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "\n",
    "tracks_pl = pl.from_pandas(filtered_reduced_trackfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that Olivia, EPAC_18_17, has missing data. The storm has one line of data for 8/27/18 and then nothing until 8/30/18. After discussions with our sponsors, we decided to drop the 8/27/18 - 8/29/18 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainband_count_pl = rainband_count_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")\n",
    "\n",
    "rainband_w_time_pl = rainband_w_time_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")\n",
    "\n",
    "innercore_count_pl = innercore_count_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")\n",
    "\n",
    "innercore_w_time_pl = innercore_w_time_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by time\n",
    "rainband_count_pl = rainband_count_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "rainband_w_time_pl = rainband_w_time_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "\n",
    "innercore_count_pl = innercore_count_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "innercore_w_time_pl = innercore_w_time_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "\n",
    "tracks_pl = tracks_pl.sort([\"year\", \"month\", \"day\", \"hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our WWLLN lightning data and TC trackfile data operate on different time intervals (trackfile data has more regular intervals, while WWLLN lightning data represents the exact time a lightning event occurs), we will join the datasets using the \"nearest\" strategy and the `join_asof` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the binned count datasets using the nearest strategy\n",
    "innercore_joined = innercore_count_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],\n",
    "    strategy=\"nearest\",  # \"backward\" (default) or \"forward\"\n",
    "    tolerance=24\n",
    ")\n",
    "\n",
    "rainband_joined = rainband_count_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],\n",
    "    strategy=\"nearest\"  # \"backward\" (default) or \"forward\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the ungrouped datasets using the nearest strategy\n",
    "innercore_joined_w_time = innercore_w_time_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],  # Optional: match on multiple keys\n",
    "    strategy=\"nearest\"  # \"backward\" (default) or \"forward\"\n",
    ")\n",
    "\n",
    "rainband_joined_w_time = rainband_w_time_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],  # Optional: match on multiple keys\n",
    "    strategy=\"nearest\",  # \"backward\" (default) or \"forward\"\n",
    "    tolerance=24\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check for nulls in our datasets. Start by converting the data back to pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to pandas to check nulls\n",
    "innercore_timebin_joined = innercore_joined.to_pandas()\n",
    "rainband_timebin_joined = rainband_joined.to_pandas()\n",
    "\n",
    "innercore_joined_w_time = innercore_joined_w_time.to_pandas()\n",
    "rainband_joined_w_time = rainband_joined_w_time.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to check for nulls in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                    0\n",
       "month                                   0\n",
       "day                                     0\n",
       "hour                                    0\n",
       "minute                                  0\n",
       "sec                                     0\n",
       "lat                                     0\n",
       "lon                                     0\n",
       "distance_from_storm_center_km_east      0\n",
       "distance_from_storm_center_km_north     0\n",
       "storm_code                              0\n",
       "storm_name                              0\n",
       "hypotenuse_disance_from_storm_center    0\n",
       "inner_core_ind                          0\n",
       "rainband_ind                            0\n",
       "datetime                                0\n",
       "time_bin                                0\n",
       "lat_right                               0\n",
       "lon_right                               0\n",
       "pressure                                0\n",
       "knots                                   0\n",
       "storm_name_right                        0\n",
       "category                                0\n",
       "basin                                   0\n",
       "minute_right                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_joined_w_time.isnull().sum()\n",
    "#innercore_joined_w_time[innercore_joined_w_time.isna().any(axis=1)]['storm_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_bin</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>lightning_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "      <th>minute_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-13 21:00:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-11-13 21:30:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-11-13 22:00:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-11-13 22:30:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-13 23:00:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187115</th>\n",
       "      <td>2020-11-26 05:30:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187116</th>\n",
       "      <td>2020-11-26 05:00:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187117</th>\n",
       "      <td>2020-11-26 06:30:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187118</th>\n",
       "      <td>2020-11-26 06:00:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187119</th>\n",
       "      <td>2020-11-26 07:00:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187120 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time_bin storm_code  lightning_count  year  month  day  \\\n",
       "0      2009-11-13 21:00:00  SHEM_10_1                1  2009     11   13   \n",
       "1      2009-11-13 21:30:00  SHEM_10_1                0  2009     11   13   \n",
       "2      2009-11-13 22:00:00  SHEM_10_1                0  2009     11   13   \n",
       "3      2009-11-13 22:30:00  SHEM_10_1                1  2009     11   13   \n",
       "4      2009-11-13 23:00:00  SHEM_10_1                0  2009     11   13   \n",
       "...                    ...        ...              ...   ...    ...  ...   \n",
       "187115 2020-11-26 05:30:00    IO_20_4               12  2020     11   26   \n",
       "187116 2020-11-26 05:00:00    IO_20_4                0  2020     11   26   \n",
       "187117 2020-11-26 06:30:00    IO_20_4                2  2020     11   26   \n",
       "187118 2020-11-26 06:00:00    IO_20_4                3  2020     11   26   \n",
       "187119 2020-11-26 07:00:00    IO_20_4                2  2020     11   26   \n",
       "\n",
       "        hour  minute   lat   lon  pressure  knots storm_name  category basin  \\\n",
       "0         21       0 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "1         21      30 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "2         22       0 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "3         22      30 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "4         23       0 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "...      ...     ...   ...   ...       ...    ...        ...       ...   ...   \n",
       "187115     5      30  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187116     5       0  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187117     6      30  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187118     6       0  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187119     7       0  13.1  79.9       989     45      Nivar         1    IO   \n",
       "\n",
       "        minute_right  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "...              ...  \n",
       "187115             0  \n",
       "187116             0  \n",
       "187117             0  \n",
       "187118             0  \n",
       "187119             0  \n",
       "\n",
       "[187120 rows x 16 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Current Category and Intensification Change\n",
    "After joining the WWLLN lightning events to its nearest wind and pressure data, we will then calculate the intensity change and category at the time of each lightning event. Intensity change is defined as the difference between the current wind speed and the wind speed 24 hours later. If the exact 24 hour timestamp cannot be found, we will compare with the closest timestamp to 24 hours. The current category of the TC at the time of the lightning event is evaluated based on the current wind speed. Once again, we refer to the [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php).\n",
    "\n",
    "**Intensification Change Bins**\n",
    "| Intensification Stage | Change in Winds (Knots) in 24 Hours (Jiang and Ramirez, 2013)|\n",
    "| --------------------- | ----------------------|\n",
    "| Weakening | <-30 to -10 |\n",
    "| Neutral | -10 to 10 |\n",
    "| Intensifying | 10 to >30 |\n",
    "\n",
    "**TC Category Bins**\n",
    "| TC Category | Sustained Winds (knots) | \n",
    "|  ---------- | ------------|\n",
    "| 1 | 64-82 kt |\n",
    "| 2 | 83-95 kt |\n",
    "| 3 | 96-112 kt |\n",
    "| 4 | 113-136 kt |\n",
    "| 5 | 137 kt or higher | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate 24-hour intensity change\n",
    "\n",
    "def knot_category(row):\n",
    "    if 64 <= row['knots'] < 83:\n",
    "        return 1\n",
    "    elif 83 <= row['knots'] < 96:\n",
    "        return 2\n",
    "    elif 96 <= row['knots'] < 113:\n",
    "        return 3\n",
    "    elif 113 <= row['knots'] < 136:\n",
    "        return 4\n",
    "    elif row['knots'] >= 137:\n",
    "        return 5\n",
    "    else:\n",
    "        return 'Unidentified'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensification(row):\n",
    "    if row['24_hour_knots_diff'] < -30:\n",
    "        return 'Rapidly Weakening'   \n",
    "    elif -30 <= row['24_hour_knots_diff'] < -10:\n",
    "        return 'Weakening'\n",
    "    elif -10 <= row['24_hour_knots_diff'] < 10:\n",
    "        return 'Neutral'\n",
    "    elif 10 <= row['24_hour_knots_diff'] < 30:\n",
    "        return 'Intensifying'\n",
    "    elif 30 <= row['24_hour_knots_diff']:\n",
    "        return 'Rapidly Intensifying'    \n",
    "    else:\n",
    "        return 'Unidentified'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_bin                        0\n",
       "storm_code                      0\n",
       "lightning_count                 0\n",
       "year                            0\n",
       "month                           0\n",
       "day                             0\n",
       "hour                            0\n",
       "minute                          0\n",
       "lat                             0\n",
       "lon                             0\n",
       "pressure                        0\n",
       "knots                           0\n",
       "storm_name                      0\n",
       "category                        0\n",
       "basin                           0\n",
       "minute_right                    0\n",
       "24_hour_knots_diff          22625\n",
       "24_hour_pressure_diff       22625\n",
       "TC_Category                     0\n",
       "Intensification_Category        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Sort by 'storm_code' and 'time_bin'\n",
    "innercore_timebin_joined = innercore_timebin_joined.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Calculate the 24-hour difference for the 'knots' column (using 'shift' to get the previous 24-hour value)\n",
    "# innercore_timebin_joined['24_hour_knots_diff'] = innercore_timebin_joined['knots'] - innercore_timebin_joined.groupby('storm_code')['knots'].shift(periods=48)\n",
    "# innercore_timebin_joined['24_hour_pressure_diff'] = innercore_timebin_joined['pressure'] - innercore_timebin_joined.groupby('storm_code')['pressure'].shift(periods=48)\n",
    "\n",
    "innercore_timebin_joined['24_hour_knots_diff'] = innercore_timebin_joined.groupby('storm_code')['knots'].shift(periods=-48) - innercore_timebin_joined['knots']\n",
    "innercore_timebin_joined['24_hour_pressure_diff'] = innercore_timebin_joined.groupby('storm_code')['pressure'].shift(periods=-48) - innercore_timebin_joined['pressure']\n",
    "\n",
    "# Save or display the DataFrame\n",
    "innercore_timebin_joined.groupby('storm_code')['24_hour_knots_diff'].max()\n",
    "innercore_timebin_joined.groupby('storm_code')['24_hour_pressure_diff'].max()\n",
    "\n",
    "innercore_timebin_joined.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_bin                     0\n",
       "storm_code                   0\n",
       "lightning_count              0\n",
       "year                         0\n",
       "month                        0\n",
       "day                          0\n",
       "hour                         0\n",
       "minute                       0\n",
       "lat                          0\n",
       "lon                          0\n",
       "pressure                     0\n",
       "knots                        0\n",
       "storm_name                   0\n",
       "category                     0\n",
       "basin                        0\n",
       "minute_right                 0\n",
       "24_hour_knots_diff       22656\n",
       "24_hour_pressure_diff    22656\n",
       "dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Sort by 'storm_code' and 'time_bin'\n",
    "rainband_timebin_joined = rainband_timebin_joined.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Calculate the 24-hour difference for the 'knots' column (using 'shift' to get the previous 24-hour value)\n",
    "rainband_timebin_joined['24_hour_knots_diff'] = rainband_timebin_joined.groupby('storm_code')['knots'].shift(periods=-48) - rainband_timebin_joined['knots']\n",
    "rainband_timebin_joined['24_hour_pressure_diff'] = rainband_timebin_joined.groupby('storm_code')['pressure'].shift(periods=-48) - rainband_timebin_joined['pressure']\n",
    "\n",
    "# Save or display the DataFrame\n",
    "rainband_timebin_joined.groupby('storm_code')['24_hour_knots_diff'].max()\n",
    "rainband_timebin_joined.groupby('storm_code')['24_hour_pressure_diff'].max()\n",
    "\n",
    "rainband_timebin_joined.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "innercore_timebin_joined['TC_Category'] = innercore_timebin_joined.apply(knot_category, axis=1)\n",
    "innercore_timebin_joined['Intensification_Category'] = innercore_timebin_joined.apply(intensification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basin  TC_Category \n",
       "ATL    Unidentified    29251\n",
       "       1                6162\n",
       "       2                2483\n",
       "       3                1606\n",
       "       4                1366\n",
       "       5                 276\n",
       "CPAC   Unidentified     2737\n",
       "       1                 510\n",
       "       2                 154\n",
       "       4                 102\n",
       "       3                  36\n",
       "       5                  18\n",
       "EPAC   Unidentified    22597\n",
       "       1                4433\n",
       "       2                2166\n",
       "       3                2101\n",
       "       4                1721\n",
       "       5                 112\n",
       "IO     Unidentified     5537\n",
       "       1                1236\n",
       "       2                 426\n",
       "       4                 416\n",
       "       3                 406\n",
       "       5                  24\n",
       "SHEM   Unidentified    31142\n",
       "       1                6503\n",
       "       2                3728\n",
       "       3                2638\n",
       "       4                2217\n",
       "       5                 300\n",
       "WPAC   Unidentified    31435\n",
       "       1                8483\n",
       "       2                4640\n",
       "       4                4478\n",
       "       3                3880\n",
       "       5                1800\n",
       "Name: TC_Category, dtype: int64"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined.groupby('basin')['TC_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basin  Intensification_Category\n",
       "ATL    Neutral                     19954\n",
       "       Intensifying                10855\n",
       "       Unidentified                 4032\n",
       "       Weakening                    3376\n",
       "       Rapidly Intensifying         2323\n",
       "       Rapidly Weakening             604\n",
       "CPAC   Neutral                      1569\n",
       "       Intensifying                 1057\n",
       "       Weakening                     363\n",
       "       Unidentified                  336\n",
       "       Rapidly Intensifying          120\n",
       "       Rapidly Weakening             112\n",
       "EPAC   Neutral                     12737\n",
       "       Intensifying                 9033\n",
       "       Unidentified                 3936\n",
       "       Weakening                    3665\n",
       "       Rapidly Intensifying         2911\n",
       "       Rapidly Weakening             848\n",
       "IO     Neutral                      2709\n",
       "       Intensifying                 2290\n",
       "       Unidentified                 1217\n",
       "       Rapidly Intensifying          739\n",
       "       Weakening                     648\n",
       "       Rapidly Weakening             442\n",
       "SHEM   Neutral                     14884\n",
       "       Intensifying                13588\n",
       "       Unidentified                 6096\n",
       "       Weakening                    5429\n",
       "       Rapidly Intensifying         4652\n",
       "       Rapidly Weakening            1879\n",
       "WPAC   Intensifying                18007\n",
       "       Neutral                     15020\n",
       "       Unidentified                 7008\n",
       "       Rapidly Intensifying         6516\n",
       "       Weakening                    5883\n",
       "       Rapidly Weakening            2282\n",
       "Name: Intensification_Category, dtype: int64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined.groupby('basin')['Intensification_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate current category based on current wind speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122699"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((innercore_timebin_joined['knots'] < 64) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_bin</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>lightning_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "      <th>minute_right</th>\n",
       "      <th>24_hour_knots_diff</th>\n",
       "      <th>24_hour_pressure_diff</th>\n",
       "      <th>TC_Category</th>\n",
       "      <th>Intensification_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>2010-06-21 07:00:00</td>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>1011</td>\n",
       "      <td>20</td>\n",
       "      <td>Alex</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>2010-06-21 07:30:00</td>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>1011</td>\n",
       "      <td>20</td>\n",
       "      <td>Alex</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5869</th>\n",
       "      <td>2010-06-21 08:00:00</td>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>1011</td>\n",
       "      <td>20</td>\n",
       "      <td>Alex</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>2010-06-21 08:30:00</td>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>1011</td>\n",
       "      <td>20</td>\n",
       "      <td>Alex</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>2010-06-21 09:00:00</td>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-67.6</td>\n",
       "      <td>1011</td>\n",
       "      <td>20</td>\n",
       "      <td>Alex</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179001</th>\n",
       "      <td>2020-08-25 13:00:00</td>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>125.1</td>\n",
       "      <td>948</td>\n",
       "      <td>95</td>\n",
       "      <td>Bavi</td>\n",
       "      <td>3</td>\n",
       "      <td>WPAC</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Unidentified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179000</th>\n",
       "      <td>2020-08-25 13:30:00</td>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>30.5</td>\n",
       "      <td>125.1</td>\n",
       "      <td>948</td>\n",
       "      <td>95</td>\n",
       "      <td>Bavi</td>\n",
       "      <td>3</td>\n",
       "      <td>WPAC</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Unidentified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179004</th>\n",
       "      <td>2020-08-25 14:00:00</td>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>125.1</td>\n",
       "      <td>948</td>\n",
       "      <td>95</td>\n",
       "      <td>Bavi</td>\n",
       "      <td>3</td>\n",
       "      <td>WPAC</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Unidentified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179003</th>\n",
       "      <td>2020-08-25 14:30:00</td>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>30.5</td>\n",
       "      <td>125.1</td>\n",
       "      <td>948</td>\n",
       "      <td>95</td>\n",
       "      <td>Bavi</td>\n",
       "      <td>3</td>\n",
       "      <td>WPAC</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Unidentified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179008</th>\n",
       "      <td>2020-08-25 15:00:00</td>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>124.8</td>\n",
       "      <td>943</td>\n",
       "      <td>100</td>\n",
       "      <td>Bavi</td>\n",
       "      <td>3</td>\n",
       "      <td>WPAC</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Unidentified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187120 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time_bin storm_code  lightning_count  year  month  day  \\\n",
       "5865   2010-06-21 07:00:00   ATL_10_1                1  2010      6   21   \n",
       "5864   2010-06-21 07:30:00   ATL_10_1                0  2010      6   21   \n",
       "5869   2010-06-21 08:00:00   ATL_10_1                0  2010      6   21   \n",
       "5870   2010-06-21 08:30:00   ATL_10_1                0  2010      6   21   \n",
       "5874   2010-06-21 09:00:00   ATL_10_1                0  2010      6   21   \n",
       "...                    ...        ...              ...   ...    ...  ...   \n",
       "179001 2020-08-25 13:00:00  WPAC_20_9                2  2020      8   25   \n",
       "179000 2020-08-25 13:30:00  WPAC_20_9                0  2020      8   25   \n",
       "179004 2020-08-25 14:00:00  WPAC_20_9                2  2020      8   25   \n",
       "179003 2020-08-25 14:30:00  WPAC_20_9                1  2020      8   25   \n",
       "179008 2020-08-25 15:00:00  WPAC_20_9                1  2020      8   25   \n",
       "\n",
       "        hour  minute   lat    lon  pressure  knots storm_name  category basin  \\\n",
       "5865       7       0  13.4  -66.5      1011     20       Alex         2   ATL   \n",
       "5864       7      30  13.4  -66.5      1011     20       Alex         2   ATL   \n",
       "5869       8       0  13.4  -66.5      1011     20       Alex         2   ATL   \n",
       "5870       8      30  13.4  -66.5      1011     20       Alex         2   ATL   \n",
       "5874       9       0  14.0  -67.6      1011     20       Alex         2   ATL   \n",
       "...      ...     ...   ...    ...       ...    ...        ...       ...   ...   \n",
       "179001    13       0  30.5  125.1       948     95       Bavi         3  WPAC   \n",
       "179000    13      30  30.5  125.1       948     95       Bavi         3  WPAC   \n",
       "179004    14       0  30.5  125.1       948     95       Bavi         3  WPAC   \n",
       "179003    14      30  30.5  125.1       948     95       Bavi         3  WPAC   \n",
       "179008    15       0  31.3  124.8       943    100       Bavi         3  WPAC   \n",
       "\n",
       "        minute_right  24_hour_knots_diff  24_hour_pressure_diff   TC_Category  \\\n",
       "5865               0                 5.0                   -1.0  Unidentified   \n",
       "5864               0                 5.0                   -1.0  Unidentified   \n",
       "5869               0                 5.0                   -1.0  Unidentified   \n",
       "5870               0                 5.0                   -1.0  Unidentified   \n",
       "5874               0                 5.0                   -1.0  Unidentified   \n",
       "...              ...                 ...                    ...           ...   \n",
       "179001             0                 NaN                    NaN             2   \n",
       "179000             0                 NaN                    NaN             2   \n",
       "179004             0                 NaN                    NaN             2   \n",
       "179003             0                 NaN                    NaN             2   \n",
       "179008             0                 NaN                    NaN             3   \n",
       "\n",
       "       Intensification_Category  \n",
       "5865                    Neutral  \n",
       "5864                    Neutral  \n",
       "5869                    Neutral  \n",
       "5870                    Neutral  \n",
       "5874                    Neutral  \n",
       "...                         ...  \n",
       "179001             Unidentified  \n",
       "179000             Unidentified  \n",
       "179004             Unidentified  \n",
       "179003             Unidentified  \n",
       "179008             Unidentified  \n",
       "\n",
       "[187120 rows x 20 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "innercore_timebin_joined.to_csv(\"innercore_timebin_joined.csv\", index=False)\n",
    "rainband_timebin_joined.to_csv(\"rainband_timebin_joined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
