{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data - Creating and Cleaning Consolidated Data Files\n",
    "\n",
    "This notebook includes the following sections:\n",
    "- Combining Files\n",
    "- Cleaning and Filtering\n",
    "- Binning the Lightning Data\n",
    "- Joining the Datasets\n",
    "- Calculating Current Category and Intensification Change\n",
    "\n",
    "This notebook should be executed after the `data_file_cleaning.ipynb` notebook. \n",
    "\n",
    "In the Combining Files section, we use the Google Drive API to grab and combine all the separate files in the folder where our data lives. We also parse the tropical cyclone (TC) ID and name from each of the file names to add as columns. This section will create intermediate files in a directory called `processed_files`, and `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt` in a directory called `combined_files` for use in the next section. The `processed_files` directory and its contents can be deleted after completion of this section.\n",
    "\n",
    "In the Cleaning and Filtering section, we perform some post-processing on the data by adding column headers, filtering to tropical cyclones that are category 1 or higher, and calculating the direct distance of each lightning strike to the TC storm center. This will create additional `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files. \n",
    "\n",
    "The Binning the Lightning Data section creates 30 minute bins for use in evaluating lightning burst thresholds and comparison with TC wind and pressure data. This section yields `WWLLN_innercore.csv`, `WWLLN_rainband.csv`, `WWLLN_innercore_w_time.csv`, `WWLLN_innercore_timebin_count.csv`,  `WWLLN_rainband_timebin_count.csv`, `WWLLN_rainband_w_time.csv`.\n",
    "\n",
    "In Joining the Datasets, we join the WWLLN datasets with the trackfile dataset. We join each 30-minute bin with the nearest wind and pressure data. This section does not create any files.\n",
    "\n",
    "The last part of this notebook, Calculating Current Category and Intensification Change, calculates category and intensification change bins for use in the burst threshold analysis. This portion will create the `innercore_joined.csv`, `innercore_joined_w_time.csv`, `rainband_joined.csv`, `rainband_joined_w_time.csv` files for use in analysis.\n",
    "\n",
    "This notebook outputs the following files:\n",
    "- `combined_files/Combined_Reduced_Trackfile.txt`\n",
    "- `combined_files/Combined_WWLLN_Locations.txt`\n",
    "- `Filtered_Reduced_Trackfile.csv`\n",
    "- `Filtered_WWLLN_Locations.txt`\n",
    "- `WWLLN_innercore.csv`\n",
    "- `WWLLN_rainband.csv`\n",
    "- `WWLLN_innercore_w_time.csv`\n",
    "- `WWLLN_innercore_timebin_count.csv`\n",
    "- `WWLLN_rainband_timebin_count.csv`\n",
    "- `WWLLN_rainband_w_time.csv`\n",
    "- `innercore_joined.csv`*\n",
    "- `innercore_joined_w_time.csv`\n",
    "- `rainband_joined.csv`*\n",
    "- `rainband_joined_w_time.csv`\n",
    "\n",
    "*denotes files used in the subsequent lightning burst threshold analysis.\n",
    "\n",
    "### Combining Files\n",
    "We use the [Google Drive API](https://developers.google.com/drive/api/guides/about-sdk) to download the files previously uploaded in `data_upload.ipynb` to consolidate the individual files. The first half of the code works if the Google Drive API is already set up (refer to instructions in `data_upload.ipynb`). The code after we create the list of files is not dependent on the Google Drive API. This section will create intermediate files in a directory called `processed_files`, and `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt` in a directory called `combined_files` for use in the next section. The `processed_files` directory and its contents can be deleted after completion of this section.\n",
    "\n",
    "Let's start by installing necessary packages and then importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import os\n",
    "import polars as pl\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from io import BytesIO\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the function in `data_file_cleaning.ipynb`, we use the following function to authenticate the Google Drive API. This will open a browser to perform the authentication process. \n",
    "\n",
    "Check if a `token.pickle` file already exists before running the following code. If the file exists, it is recommended to delete it before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=389849867563-4uggnm57nqe52156v32gj1lkosoqpoem.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A37635%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=GyfvJGpr2mtpuRzI6AP8xi3h1k3QIH&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# Scopes for accessing Google Drive\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Authenticate and create the service object\n",
    "def authenticate_drive_api():\n",
    "    creds = None\n",
    "    # Token file for saving the authentication\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no credentials, perform authentication\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)  # Ensure 'credentials.json' is downloaded from Google API Console\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for future use\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Initialize the service object\n",
    "service = authenticate_drive_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function grabs the list of all files in a specified folder that are not trashed and stores them into a list. Each file has an ID and name attribute that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(folder_id):\n",
    "    # Query to find files in the specified folder\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    files = []\n",
    "\n",
    "    # List files in the folder and append to list\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields='nextPageToken, files(id, name)',\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        files += response.get('files', [])\n",
    "\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to find files in the specified folder. The folder ID can be found as the string after the \"folders/\" part of the URL for the Google Drive folder. This will give us a list of files to iterate through for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the folder\n",
    "folder_id = '14idmMBbM5xXZg4b61iINHbBTl2z4yLeD'\n",
    "files = find_files(folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split out the tropical cyclone ID and name from each of the files to add as a separate column. We then save the files in the `processed_files` directory for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file to add cyclone ID and name as columns\n",
    "# Directory to save the processed files locally\n",
    "output_dir = \"processed_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "\n",
    "    # Extract the cyclone ID and name from the filename\n",
    "    cyclone_id = '_'.join(file_name.split('_')[:3])\n",
    "    cyclone_name = file_name.split('_')[3]\n",
    "\n",
    "    # Download the file content\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_stream = BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_stream, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "    file_stream.seek(0)\n",
    "    content = file_stream.read().decode('utf-8')\n",
    "\n",
    "    # Add the cyclone id and name as a new column using Polars\n",
    "    df = pl.read_csv(BytesIO(content.encode('utf-8')),separator='\\t', has_header=False)\n",
    "    df = df.with_columns([\n",
    "    pl.lit(cyclone_id).alias(\"cyclone_id\"),\n",
    "    pl.lit(cyclone_name).alias(\"cyclone_name\")\n",
    "    ])\n",
    "\n",
    "    # Save the modified DataFrame locally\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    df.write_csv(output_file_path, separator='\\t',include_header=False)\n",
    "\n",
    "    print(f\"Processed and saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine each of the trackfiles in the `processed_files` folder into one file, and each of the WWLLN location files into one file. This will give us two output files in the `combined_files` folder - `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt`. We will use these files as the basis for the next portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining 992 files for pattern 'Reduced_Trackfile'...\n",
      "Combined file saved: combined_files/Combined_Reduced_Trackfile.txt\n",
      "Combining 994 files for pattern 'WWLLN_Locations'...\n",
      "Combined file saved: combined_files/Combined_WWLLN_Locations.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Directories for processed files and output\n",
    "input_dir = \"processed_files\"\n",
    "output_dir = \"combined_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File patterns to combine\n",
    "patterns = {\n",
    "    \"Reduced_Trackfile\": os.path.join(input_dir, \"*Reduced_Trackfile*.txt\"),\n",
    "    \"WWLLN_Locations\": os.path.join(input_dir, \"*WWLLN_Locations*.txt\")\n",
    "}\n",
    "\n",
    "# Combine files based on patterns\n",
    "for pattern_name, pattern_path in patterns.items():\n",
    "    combined_content = []\n",
    "    output_file_path = os.path.join(output_dir, f\"Combined_{pattern_name}.txt\")\n",
    "\n",
    "    # Find all matching files\n",
    "    matching_files = glob.glob(pattern_path)\n",
    "    print(f\"Combining {len(matching_files)} files for pattern '{pattern_name}'...\")\n",
    "\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for file_path in matching_files:\n",
    "            with open(file_path, \"r\") as input_file:\n",
    "                for line in input_file:\n",
    "                    output_file.write(line)\n",
    "\n",
    "    print(f\"Combined file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Filtering\n",
    "In this section we add a column header to the files and filter down to TCs that are category 1 and above. Category 1 is defined using the [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php), where the maximum sustained wind speed is between 64-82 kt. We calculate each TC's category using the Saffir-Simpson Scale and save it in a new column. We then calculate the direct distance of each lightning strike to the storm center and denote it as inner core or rainband. This section outputs `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files.\n",
    "\n",
    "Start by importing the necessary libraries and files created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trackfile and WWLLN file from google drive\n",
    "track_file = pd.read_csv(\"Combined_Reduced_Trackfile.txt\", sep=\"\\t\")\n",
    "track_file = track_file.drop(track_file.columns[8], axis=1)\n",
    "\n",
    "# Use chuck due to the large file size\n",
    "chunksize = 100000  \n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    \"Combined_WWLLN_Locations.txt\",\n",
    "    delim_whitespace=True,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add headers to the two dataframes for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file.columns = ['year', 'month', 'day','hour','lat','lon','pressure', 'knots', 'storm_code', 'storm_name']\n",
    "locations_WWLLN.columns = ['year', 'month', 'day', 'hour', 'min', 'sec','lat','lon','distance_from_storm_center_km_east', 'distance_from_storm_center_km_north', 'storm_code','storm_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the trackfile data by creating the list of storm codes that meet the category 1 or higher requirement. We will use this list to filter the wind speed/pressure data as well as the lightning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>max_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL_10_10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  max_wind_speed\n",
       "0   ATL_10_1              85\n",
       "1  ATL_10_10              30\n",
       "2  ATL_10_11             135\n",
       "3  ATL_10_12             120\n",
       "4  ATL_10_13             105"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the max wind speed for each storm code\n",
    "\n",
    "max_wind_speed = track_file.groupby('storm_code').agg(\n",
    "    max_wind_speed=('knots', 'max')\n",
    ").reset_index()\n",
    "max_wind_speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ATL_10_14</td>\n",
       "      <td>1</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  category basin\n",
       "0   ATL_10_1         2   ATL\n",
       "2  ATL_10_11         4   ATL\n",
       "3  ATL_10_12         4   ATL\n",
       "4  ATL_10_13         3   ATL\n",
       "5  ATL_10_14         1   ATL"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by max >= 64 knots\n",
    "storm_filter = max_wind_speed[max_wind_speed[\"max_wind_speed\"] >= 64].copy()\n",
    "\n",
    "# calculate the TC category using the max wind speed\n",
    "storm_filter[\"category\"] = storm_filter[\"max_wind_speed\"].apply(\n",
    "    lambda x: 1 if 64 <= x <= 82 else (2 if 82 < x <= 95 else (3 if 95 < x <= 112 else (4 if 112 < x <= 136 else (5 if x > 136 else 0))))\n",
    ")\n",
    "storm_filter = storm_filter[[\"storm_code\", \"category\"]]\n",
    "\n",
    "# strip the basin from the storm code\n",
    "storm_filter[\"basin\"] = storm_filter[\"storm_code\"].str.extract(r\"^(.*?)_\")\n",
    "storm_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of TCs: 982, category 1 or higher number of TCs: 473\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall number of TCs: {len(max_wind_speed)}, category 1 or higher number of TCs: {len(storm_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  \n",
       "0         2   ATL  \n",
       "1         2   ATL  \n",
       "2         2   ATL  \n",
       "3         2   ATL  \n",
       "4         2   ATL  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the trackfile data by the storm filter\n",
    "track_file_filtered = track_file[track_file[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "# join the category column by storm code\n",
    "track_file_filtered = pd.merge(track_file_filtered, storm_filter, how='inner', on='storm_code')\n",
    "\n",
    "track_file_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a csv file for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file_filtered.to_csv('Filtered_Reduced_Trackfile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on the WWLLN dataset. Start by filtering the WWLLN dataset by the storm filter created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter WWLLN dataset by the storm filter\n",
    "locations_WWLLN_filtered = locations_WWLLN[locations_WWLLN[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "locations_WWLLN_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the direct distance of each lightning instance from the storm center using a simple triangle calculation. We have the north and east distances from center, so we use the Pythagorean theorem to simply calculate the missing hypotenuse. We also create an indicator for inner core lightning and another for rainband lightning. Inner core lightning is defined as within 100km of storm center, while rainband lightning is defined as between 200-400km of storm center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered['hypotenuse_disance_from_storm_center'] = np.sqrt(locations_WWLLN_filtered['distance_from_storm_center_km_east'] ** 2 +locations_WWLLN['distance_from_storm_center_km_north'] ** 2)\n",
    "locations_WWLLN_filtered[\"inner_core_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if x <= 100 else 0\n",
    ")\n",
    "locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if (x >= 200 and x <= 400) else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a txt file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered.to_csv(\"Filtered_WWLLN_Locations.txt\", sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning the Lightning Data\n",
    "To compare the lightning bursts with TC wind and pressure data, we must bin and join the trackfile and WWLLN datasets. Because of the size of the data we're working with, we can restart the kernel here to keep memory use more optimal. \n",
    "\n",
    "We will first bin the lightning data into 30-minute bins, yielding 48 bins a day. Each bin contains a count of lightning events during that timeframe. We will then join the binned data to the trackfile data.\n",
    "\n",
    "Let's start by importing necessary libraries and files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the WWLLN file from earlier, but filtered to keep either rainband or innercore\n",
    "\n",
    "chunksize = 100000  \n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    r\"C:\\Users\\user\\Desktop\\25 WI\\Filtered_WWLLN_Locations.txt\",\n",
    "    delim_whitespace=True,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN_filtered_ = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial analysis, we are only interested in either inner core or rainband lighting. We drop all other lightning events that are not categorized as either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the inner core or rainband lightning events using the indicator columns\n",
    "locations_WWLLN_filtered_inner_rainband = locations_WWLLN_filtered_[\n",
    "    ~((locations_WWLLN_filtered_[\"rainband_ind\"] == 0) & (locations_WWLLN_filtered_[\"inner_core_ind\"] == 0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out just the inner core data\n",
    "locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_inner_rainband[locations_WWLLN_filtered_inner_rainband['inner_core_ind'] == 1]\n",
    "# print(locations_WWLLN_filtered_innercore.shape)\n",
    "# (2611951, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out just the rainband data\n",
    "locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_inner_rainband[locations_WWLLN_filtered_inner_rainband['rainband_ind'] == 1]\n",
    "# print(locations_WWLLN_filtered_rainband.shape)\n",
    "# (10146702, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the inner core and rainband datasets as separate csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore.csv\", index=False)\n",
    "locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restart the kernel again here to free up memory. Next, we will create 30-minute bins for both the inner core and rainband datasets.\n",
    "\n",
    "**!!! need to fix missing bins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the files we just exported\n",
    "locations_WWLLN_filtered_innercore = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore.csv\")\n",
    "locations_WWLLN_filtered_rainband = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prev Version, keep this for record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8260\\4028445333.py:18: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>time_bin</th>\n",
       "      <th>lightning_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-21 07:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 14:30:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 15:00:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 16:30:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2010-06-23 22:30:00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58145</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 12:30:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58146</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 13:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58147</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 14:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58148</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 14:30:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58149</th>\n",
       "      <td>WPAC_20_9</td>\n",
       "      <td>2020-08-25 15:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58150 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      storm_code            time_bin  lightning_count\n",
       "0       ATL_10_1 2010-06-21 07:00:00                1\n",
       "1       ATL_10_1 2010-06-23 14:30:00                1\n",
       "2       ATL_10_1 2010-06-23 15:00:00                3\n",
       "3       ATL_10_1 2010-06-23 16:30:00                2\n",
       "4       ATL_10_1 2010-06-23 22:30:00                4\n",
       "...          ...                 ...              ...\n",
       "58145  WPAC_20_9 2020-08-25 12:30:00                2\n",
       "58146  WPAC_20_9 2020-08-25 13:00:00                2\n",
       "58147  WPAC_20_9 2020-08-25 14:00:00                2\n",
       "58148  WPAC_20_9 2020-08-25 14:30:00                1\n",
       "58149  WPAC_20_9 2020-08-25 15:00:00                1\n",
       "\n",
       "[58150 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # bin the inner core data\n",
    "# # Create a datetime column from the existing columns\n",
    "# locations_WWLLN_filtered_innercore['sec'] = locations_WWLLN_filtered_innercore['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# locations_WWLLN_filtered_innercore['datetime'] = pd.to_datetime(locations_WWLLN_filtered_innercore['year'].astype(str) + '-' +\n",
    "#                                  locations_WWLLN_filtered_innercore['month'].astype(str).str.zfill(2) + '-' +\n",
    "#                                  locations_WWLLN_filtered_innercore['day'].astype(str).str.zfill(2) + ' ' +\n",
    "#                                  locations_WWLLN_filtered_innercore['hour'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_innercore['min'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_innercore['sec'].astype(str).str.zfill(2))\n",
    "\n",
    "# # Define a function to apply the 30-minute binning for each storm_code group\n",
    "# def add_time_bin(group):\n",
    "#     group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "#     return group\n",
    "\n",
    "# # Group by storm_code and apply the binning function\n",
    "# locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# # Print the resulting DataFrame with the new 'time_bin' column\n",
    "# locations_WWLLN_filtered_innercore.head()\n",
    "\n",
    "# # group by bins and get the count per 30-minute bin\n",
    "# locations_WWLLN_filtered_innercore_grouped = locations_WWLLN_filtered_innercore.groupby(['storm_code', 'time_bin'])\n",
    "# locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(by = ['storm_code', 'time_bin'])\n",
    "# locations_WWLLN_filtered_innercore_timebin\n",
    "# # export both the ungrouped and grouped datasets to csv files\n",
    "# # locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore_w_time.csv\", index=False)\n",
    "# # locations_WWLLN_filtered_innercore_timebin.to_csv(\"WWLLN_innercore_timebin_count.csv\", index=False)\n",
    "\n",
    "\n",
    "#### Rain Band\n",
    "\n",
    "# # bin the rainband data\n",
    "# # Create a datetime column from the existing columns\n",
    "# locations_WWLLN_filtered_rainband['sec'] = locations_WWLLN_filtered_rainband['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# locations_WWLLN_filtered_rainband['datetime'] = pd.to_datetime(locations_WWLLN_filtered_rainband['year'].astype(str) + '-' +\n",
    "#                                  locations_WWLLN_filtered_rainband['month'].astype(str).str.zfill(2) + '-' +\n",
    "#                                  locations_WWLLN_filtered_rainband['day'].astype(str).str.zfill(2) + ' ' +\n",
    "#                                  locations_WWLLN_filtered_rainband['hour'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_rainband['min'].astype(str).str.zfill(2) + ':' +\n",
    "#                                  locations_WWLLN_filtered_rainband['sec'].astype(str).str.zfill(2))\n",
    "\n",
    "# # Define a function to apply the 30-minute binning for each storm_code group\n",
    "# def add_time_bin(group):\n",
    "#     group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "#     return group\n",
    "\n",
    "# # Group by storm_code and apply the binning function\n",
    "# locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# # Print the resulting DataFrame with the new 'time_bin' column\n",
    "# locations_WWLLN_filtered_rainband.head()\n",
    "# # group by bins and get the count per 30-minute bin\n",
    "# locations_WWLLN_filtered_rainband_grouped = locations_WWLLN_filtered_rainband.groupby(['storm_code', 'time_bin'])\n",
    "# locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.sort_values(by = ['storm_code', 'time_bin'])\n",
    "\n",
    "# # export both the ungrouped and grouped datasets to csv files\n",
    "# locations_WWLLN_filtered_rainband_timebin.to_csv(\"WWLLN_rainband_timebin_count.csv\", index=False)\n",
    "# locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband_w_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8260\\1105446163.py:20: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n"
     ]
    }
   ],
   "source": [
    "# Ensure sec column is valid\n",
    "locations_WWLLN_filtered_innercore['sec'] = locations_WWLLN_filtered_innercore['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# Create a datetime column\n",
    "locations_WWLLN_filtered_innercore['datetime'] = pd.to_datetime(\n",
    "    locations_WWLLN_filtered_innercore['year'].astype(str) + '-' +\n",
    "    locations_WWLLN_filtered_innercore['month'].astype(str).str.zfill(2) + '-' +\n",
    "    locations_WWLLN_filtered_innercore['day'].astype(str).str.zfill(2) + ' ' +\n",
    "    locations_WWLLN_filtered_innercore['hour'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_innercore['min'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_innercore['sec'].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "# Define a function to apply the 30-minute binning for each storm_code group\n",
    "def add_time_bin(group):\n",
    "    group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "    return group\n",
    "\n",
    "# Group by storm_code and apply the binning function\n",
    "locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# Group by bins and get the count per 30-minute bin\n",
    "locations_WWLLN_filtered_innercore_grouped = locations_WWLLN_filtered_innercore.groupby(['storm_code', 'time_bin'])\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# Add missing 30-minute bins for each storm_code\n",
    "def add_missing_bins(group):\n",
    "    min_time, max_time = group['time_bin'].min(), group['time_bin'].max()\n",
    "    \n",
    "    # Create a full range of 30-minute bins for the time period of this storm\n",
    "    full_bins = pd.DataFrame({'time_bin': pd.date_range(min_time, max_time, freq='30T')})\n",
    "    \n",
    "    # Merge the full_bins with the original group\n",
    "    merged = full_bins.merge(group[['storm_code', 'time_bin', 'lightning_count']], how='left', on='time_bin')\n",
    "    \n",
    "    # Fill missing lightning_count with 0 where there is no data\n",
    "    merged['lightning_count'] = merged['lightning_count'].fillna(0).astype(int)\n",
    "    \n",
    "    # Fill missing storm_code with the first valid entry in the group\n",
    "    merged['storm_code'] = merged['storm_code'].fillna(method='ffill')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Apply the function to each storm_code\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.groupby('storm_code', group_keys=False).apply(add_missing_bins)\n",
    "\n",
    "# Sort the final result\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Print the resulting DataFrame with the new 'time_bin' and 'lightning_count' columns\n",
    "locations_WWLLN_filtered_innercore_timebin.head()\n",
    "\n",
    "# # Export both the ungrouped and grouped datasets\n",
    "locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore_w_time.csv\", index=False)\n",
    "locations_WWLLN_filtered_innercore_timebin.to_csv(\"WWLLN_innercore_timebin_count.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8260\\2887306073.py:20: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n"
     ]
    }
   ],
   "source": [
    "# Ensure sec column is valid\n",
    "locations_WWLLN_filtered_rainband['sec'] = locations_WWLLN_filtered_rainband['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "# Create a datetime column\n",
    "locations_WWLLN_filtered_rainband['datetime'] = pd.to_datetime(\n",
    "    locations_WWLLN_filtered_rainband['year'].astype(str) + '-' +\n",
    "    locations_WWLLN_filtered_rainband['month'].astype(str).str.zfill(2) + '-' +\n",
    "    locations_WWLLN_filtered_rainband['day'].astype(str).str.zfill(2) + ' ' +\n",
    "    locations_WWLLN_filtered_rainband['hour'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_rainband['min'].astype(str).str.zfill(2) + ':' +\n",
    "    locations_WWLLN_filtered_rainband['sec'].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "# Define a function to apply the 30-minute binning for each storm_code group\n",
    "def add_time_bin(group):\n",
    "    group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "    return group\n",
    "\n",
    "# Group by storm_code and apply the binning function\n",
    "locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# Group by bins and get the count per 30-minute bin\n",
    "locations_WWLLN_filtered_rainband_grouped = locations_WWLLN_filtered_rainband.groupby(['storm_code', 'time_bin'])\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_grouped.size().reset_index(name='lightning_count')\n",
    "\n",
    "# Add missing 30-minute bins for each storm_code\n",
    "def add_missing_bins(group):\n",
    "    min_time, max_time = group['time_bin'].min(), group['time_bin'].max()\n",
    "    \n",
    "    # Create a full range of 30-minute bins for the time period of this storm\n",
    "    full_bins = pd.DataFrame({'time_bin': pd.date_range(min_time, max_time, freq='30T')})\n",
    "    \n",
    "    # Merge the full_bins with the original group\n",
    "    merged = full_bins.merge(group[['storm_code', 'time_bin', 'lightning_count']], how='left', on='time_bin')\n",
    "    \n",
    "    # Fill missing lightning_count with 0 where there is no data\n",
    "    merged['lightning_count'] = merged['lightning_count'].fillna(0).astype(int)\n",
    "    \n",
    "    # Fill missing storm_code with the first valid entry in the group\n",
    "    merged['storm_code'] = merged['storm_code'].fillna(method='ffill')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Apply the function to each storm_code\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.groupby('storm_code', group_keys=False).apply(add_missing_bins)\n",
    "\n",
    "# Sort the final result\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Print the resulting DataFrame with the new 'time_bin' and 'lightning_count' columns\n",
    "locations_WWLLN_filtered_rainband_timebin.head()\n",
    "\n",
    "# # Export both the ungrouped and grouped datasets\n",
    "locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband_w_time.csv\", index=False)\n",
    "locations_WWLLN_filtered_rainband_timebin.to_csv(\"WWLLN_rainband_timebin_count.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the Datasets\n",
    "The trackfile data includes wind and pressure data at regular intervals for each TC. Since there is no function that approximates the behavior of TC wind and pressure change, we will not interpolate the data for timestamps between the intervals - rather, we will join each lightning event to the nearest wind and pressure data and perform our analysis as such. We recognize that this process is a source of error, but we believe that joining to the nearest timestamp will keep this known error as low as possible for the sake of this analysis.\n",
    "\n",
    "Restart the kernel again here to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the necessary files\n",
    "filtered_reduced_trackfile = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\Filtered_Reduced_Trackfile.csv\")\n",
    "locations_WWLLN_filtered_rainband_timebin = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband_timebin_count_updated.csv\")\n",
    "locations_WWLLN_filtered_rainband = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband_w_time_updated.csv\")\n",
    "locations_WWLLN_filtered_innercore_timebin = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore_timebin_count_updated.csv\")\n",
    "locations_WWLLN_filtered_innercore = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore_w_time_updated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the datetime columns into datetime data types to ensure that dates are represented correctly and yield an accurate join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime columns to the correct data type for inner core data\n",
    "locations_WWLLN_filtered_innercore_timebin[\"time_bin\"] = pd.to_datetime(locations_WWLLN_filtered_innercore_timebin[\"time_bin\"])\n",
    "\n",
    "locations_WWLLN_filtered_innercore_timebin[\"year\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.year\n",
    "locations_WWLLN_filtered_innercore_timebin[\"month\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.month\n",
    "locations_WWLLN_filtered_innercore_timebin[\"day\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.day\n",
    "locations_WWLLN_filtered_innercore_timebin[\"hour\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.hour\n",
    "locations_WWLLN_filtered_innercore_timebin[\"minute\"] = locations_WWLLN_filtered_innercore_timebin[\"time_bin\"].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime columns to the correct data type for rainband data\n",
    "locations_WWLLN_filtered_rainband_timebin[\"time_bin\"] = pd.to_datetime(locations_WWLLN_filtered_rainband_timebin[\"time_bin\"])\n",
    "\n",
    "locations_WWLLN_filtered_rainband_timebin[\"year\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.year\n",
    "locations_WWLLN_filtered_rainband_timebin[\"month\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.month\n",
    "locations_WWLLN_filtered_rainband_timebin[\"day\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.day\n",
    "locations_WWLLN_filtered_rainband_timebin[\"hour\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.hour\n",
    "locations_WWLLN_filtered_rainband_timebin[\"minute\"] = locations_WWLLN_filtered_rainband_timebin[\"time_bin\"].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the minute columns\n",
    "locations_WWLLN_filtered_rainband.rename(columns={'min': 'minute'}, inplace=True)\n",
    "locations_WWLLN_filtered_innercore.rename(columns={'min': 'minute'}, inplace=True)\n",
    "filtered_reduced_trackfile['minute'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  minute  \n",
       "0         2   ATL       0  \n",
       "1         2   ATL       0  \n",
       "2         2   ATL       0  \n",
       "3         2   ATL       0  \n",
       "4         2   ATL       0  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_reduced_trackfile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(\n",
    "    [\"storm_code\", \"year\", \"month\", \"day\", \"hour\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "filtered_reduced_trackfile = filtered_reduced_trackfile.sort_values(\n",
    "    [\"storm_code\", \"year\", \"month\", \"day\", \"hour\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to polars for faster processing\n",
    "rainband_count_pl = pl.from_pandas(locations_WWLLN_filtered_rainband_timebin)\n",
    "rainband_w_time_pl = pl.from_pandas(locations_WWLLN_filtered_rainband)\n",
    "rainband_w_time_pl = rainband_w_time_pl.with_columns(\n",
    "    pl.col(\"time_bin\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "\n",
    "\n",
    "innercore_count_pl = pl.from_pandas(locations_WWLLN_filtered_innercore_timebin)\n",
    "innercore_w_time_pl = pl.from_pandas(locations_WWLLN_filtered_innercore)\n",
    "innercore_w_time_pl = innercore_w_time_pl.with_columns(\n",
    "    pl.col(\"time_bin\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "\n",
    "tracks_pl = pl.from_pandas(filtered_reduced_trackfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that Olivia, EPAC_18_17, has missing data. The storm has one line of data for 8/27/18 and then nothing until 8/30/18. After discussions with our sponsors, we decided to drop the 8/27/18 - 8/29/18 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainband_count_pl = rainband_count_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")\n",
    "\n",
    "rainband_w_time_pl = rainband_w_time_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")\n",
    "\n",
    "innercore_count_pl = innercore_count_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")\n",
    "\n",
    "innercore_w_time_pl = innercore_w_time_pl.filter(\n",
    "    ~((pl.col(\"storm_code\") == \"EPAC_18_17\") & \n",
    "    (pl.col(\"time_bin\") < pl.datetime(2018, 8, 30)) )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by time\n",
    "rainband_count_pl = rainband_count_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "rainband_w_time_pl = rainband_w_time_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "\n",
    "innercore_count_pl = innercore_count_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "innercore_w_time_pl = innercore_w_time_pl.sort([\"year\", \"month\", \"day\", \"hour\"])\n",
    "\n",
    "tracks_pl = tracks_pl.sort([\"year\", \"month\", \"day\", \"hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our WWLLN lightning data and TC trackfile data operate on different time intervals (trackfile data has more regular intervals, while WWLLN lightning data represents the exact time a lightning event occurs), we will join the datasets using the \"nearest\" strategy and the `join_asof` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the binned count datasets using the nearest strategy\n",
    "innercore_joined = innercore_count_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],\n",
    "    strategy=\"nearest\",  # \"backward\" (default) or \"forward\"\n",
    "    tolerance=24\n",
    ")\n",
    "\n",
    "rainband_joined = rainband_count_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],\n",
    "    strategy=\"nearest\"  # \"backward\" (default) or \"forward\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the ungrouped datasets using the nearest strategy\n",
    "innercore_joined_w_time = innercore_w_time_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],  # Optional: match on multiple keys\n",
    "    strategy=\"nearest\"  # \"backward\" (default) or \"forward\"\n",
    ")\n",
    "\n",
    "rainband_joined_w_time = rainband_w_time_pl.join_asof(\n",
    "    tracks_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\",\"storm_code\"],  # Optional: match on multiple keys\n",
    "    strategy=\"nearest\",  # \"backward\" (default) or \"forward\"\n",
    "    tolerance=24\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check for nulls in our datasets. Start by converting the data back to pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to pandas to check nulls\n",
    "innercore_timebin_joined = innercore_joined.to_pandas()\n",
    "rainband_timebin_joined = rainband_joined.to_pandas()\n",
    "\n",
    "innercore_joined_w_time = innercore_joined_w_time.to_pandas()\n",
    "rainband_joined_w_time = rainband_joined_w_time.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to check for nulls in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                    0\n",
       "month                                   0\n",
       "day                                     0\n",
       "hour                                    0\n",
       "minute                                  0\n",
       "sec                                     0\n",
       "lat                                     0\n",
       "lon                                     0\n",
       "distance_from_storm_center_km_east      0\n",
       "distance_from_storm_center_km_north     0\n",
       "storm_code                              0\n",
       "storm_name                              0\n",
       "hypotenuse_disance_from_storm_center    0\n",
       "inner_core_ind                          0\n",
       "rainband_ind                            0\n",
       "datetime                                0\n",
       "time_bin                                0\n",
       "lat_right                               0\n",
       "lon_right                               0\n",
       "pressure                                0\n",
       "knots                                   0\n",
       "storm_name_right                        0\n",
       "category                                0\n",
       "basin                                   0\n",
       "minute_right                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_joined_w_time.isnull().sum()\n",
    "#innercore_joined_w_time[innercore_joined_w_time.isna().any(axis=1)]['storm_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_bin</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>lightning_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "      <th>minute_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-13 21:00:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-11-13 21:30:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-11-13 22:00:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-11-13 22:30:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-13 23:00:00</td>\n",
       "      <td>SHEM_10_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>Anja</td>\n",
       "      <td>3</td>\n",
       "      <td>SHEM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187115</th>\n",
       "      <td>2020-11-26 05:30:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187116</th>\n",
       "      <td>2020-11-26 05:00:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187117</th>\n",
       "      <td>2020-11-26 06:30:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187118</th>\n",
       "      <td>2020-11-26 06:00:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187119</th>\n",
       "      <td>2020-11-26 07:00:00</td>\n",
       "      <td>IO_20_4</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>989</td>\n",
       "      <td>45</td>\n",
       "      <td>Nivar</td>\n",
       "      <td>1</td>\n",
       "      <td>IO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187120 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time_bin storm_code  lightning_count  year  month  day  \\\n",
       "0      2009-11-13 21:00:00  SHEM_10_1                1  2009     11   13   \n",
       "1      2009-11-13 21:30:00  SHEM_10_1                0  2009     11   13   \n",
       "2      2009-11-13 22:00:00  SHEM_10_1                0  2009     11   13   \n",
       "3      2009-11-13 22:30:00  SHEM_10_1                1  2009     11   13   \n",
       "4      2009-11-13 23:00:00  SHEM_10_1                0  2009     11   13   \n",
       "...                    ...        ...              ...   ...    ...  ...   \n",
       "187115 2020-11-26 05:30:00    IO_20_4               12  2020     11   26   \n",
       "187116 2020-11-26 05:00:00    IO_20_4                0  2020     11   26   \n",
       "187117 2020-11-26 06:30:00    IO_20_4                2  2020     11   26   \n",
       "187118 2020-11-26 06:00:00    IO_20_4                3  2020     11   26   \n",
       "187119 2020-11-26 07:00:00    IO_20_4                2  2020     11   26   \n",
       "\n",
       "        hour  minute   lat   lon  pressure  knots storm_name  category basin  \\\n",
       "0         21       0 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "1         21      30 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "2         22       0 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "3         22      30 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "4         23       0 -11.4  71.5      1000     30       Anja         3  SHEM   \n",
       "...      ...     ...   ...   ...       ...    ...        ...       ...   ...   \n",
       "187115     5      30  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187116     5       0  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187117     6      30  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187118     6       0  13.1  79.9       989     45      Nivar         1    IO   \n",
       "187119     7       0  13.1  79.9       989     45      Nivar         1    IO   \n",
       "\n",
       "        minute_right  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "...              ...  \n",
       "187115             0  \n",
       "187116             0  \n",
       "187117             0  \n",
       "187118             0  \n",
       "187119             0  \n",
       "\n",
       "[187120 rows x 16 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Current Category and Intensification Change\n",
    "After joining the WWLLN lightning events to its nearest wind and pressure data, we will then calculate the intensity change and category at the time of each lightning event. Intensity change is defined as the difference between the current wind speed and the wind speed 24 hours later. If the exact 24 hour timestamp cannot be found, we will compare with the closest timestamp to 24 hours. The current category of the TC at the time of the lightning event is evaluated based on the current wind speed. Once again, we refer to the [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php).\n",
    "\n",
    "**Intensification Change Bins**\n",
    "| Intensification Stage | Change in Winds (Knots) in 24 Hours (Jiang and Ramirez, 2013)|\n",
    "| --------------------- | ----------------------|\n",
    "| Weakening | <-30 to -10 |\n",
    "| Neutral | -10 to 10 |\n",
    "| Intensifying | 10 to >30 |\n",
    "\n",
    "**TC Category Bins**\n",
    "| TC Category | Sustained Winds (knots) | \n",
    "|  ---------- | ------------|\n",
    "| 1 | 64-82 kt |\n",
    "| 2 | 83-95 kt |\n",
    "| 3 | 96-112 kt |\n",
    "| 4 | 113-136 kt |\n",
    "| 5 | 137 kt or higher | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate 24-hour intensity change\n",
    "\n",
    "def knot_category(row):\n",
    "    if 64 <= row['knots'] < 83:\n",
    "        return 1\n",
    "    elif 83 <= row['knots'] < 96:\n",
    "        return 2\n",
    "    elif 96 <= row['knots'] < 113:\n",
    "        return 3\n",
    "    elif 113 <= row['knots'] < 136:\n",
    "        return 4\n",
    "    elif row['knots'] >= 137:\n",
    "        return 5\n",
    "    else:\n",
    "        return 'Unidentified'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensification(row):\n",
    "    if -30 <= row['24_hour_knots_diff'] < -10:\n",
    "        return 'Weakning'\n",
    "    elif -10 <= row['24_hour_knots_diff'] < 10:\n",
    "        return 'Neutral'\n",
    "    elif 10 <= row['24_hour_knots_diff'] < 30:\n",
    "        return 'Intensifying'\n",
    "    else:\n",
    "        return 'Unidentified'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Sort by 'storm_code' and 'time_bin'\n",
    "innercore_timebin_joined = innercore_timebin_joined.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Calculate the 24-hour difference for the 'knots' column (using 'shift' to get the previous 24-hour value)\n",
    "innercore_timebin_joined['24_hour_knots_diff'] = innercore_timebin_joined['knots'] - innercore_timebin_joined.groupby('storm_code')['knots'].shift(periods=48)\n",
    "innercore_timebin_joined['24_hour_pressure_diff'] = innercore_timebin_joined['pressure'] - innercore_timebin_joined.groupby('storm_code')['pressure'].shift(periods=48)\n",
    "\n",
    "# Save or display the DataFrame\n",
    "innercore_timebin_joined.groupby('storm_code')['24_hour_knots_diff'].max()\n",
    "innercore_timebin_joined.groupby('storm_code')['24_hour_pressure_diff'].max()\n",
    "\n",
    "innercore_timebin_joined.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "storm_code\n",
       "ATL_10_1         1.0\n",
       "ATL_10_11       12.0\n",
       "ATL_10_12       26.0\n",
       "ATL_10_13        3.0\n",
       "ATL_10_14       18.0\n",
       "ATL_10_17       12.0\n",
       "ATL_10_18       16.0\n",
       "ATL_10_19        1.0\n",
       "ATL_10_20        1.0\n",
       "ATL_10_21       20.0\n",
       "ATL_10_6        16.0\n",
       "ATL_10_7        17.0\n",
       "ATL_11_12       23.0\n",
       "ATL_11_14        4.0\n",
       "ATL_11_16       13.0\n",
       "ATL_11_17       13.0\n",
       "ATL_11_18       18.0\n",
       "ATL_11_9        20.0\n",
       "ATL_12_11       19.0\n",
       "ATL_12_12        7.0\n",
       "ATL_12_13       13.0\n",
       "ATL_12_14        7.0\n",
       "ATL_12_17        2.0\n",
       "ATL_12_18       -1.0\n",
       "ATL_12_3         2.0\n",
       "ATL_12_5        15.0\n",
       "ATL_12_8        11.0\n",
       "ATL_12_9        21.0\n",
       "ATL_13_10       18.0\n",
       "ATL_13_9        19.0\n",
       "ATL_14_1         1.0\n",
       "ATL_14_3         9.0\n",
       "ATL_14_4         2.0\n",
       "ATL_14_6        23.0\n",
       "ATL_14_7         0.0\n",
       "ATL_14_8         8.0\n",
       "ATL_15_11       24.0\n",
       "ATL_15_12        0.0\n",
       "ATL_15_4        20.0\n",
       "ATL_15_6      1008.0\n",
       "ATL_16_1         8.0\n",
       "ATL_16_15       32.0\n",
       "ATL_16_16     1008.0\n",
       "ATL_16_5        22.0\n",
       "ATL_16_7        21.0\n",
       "ATL_16_9        15.0\n",
       "ATL_17_11     1008.0\n",
       "ATL_17_12       24.0\n",
       "ATL_17_13        1.0\n",
       "ATL_17_14       19.0\n",
       "ATL_17_15     1012.0\n",
       "ATL_17_16       -2.0\n",
       "ATL_17_17        0.0\n",
       "ATL_17_7         2.0\n",
       "ATL_17_8        15.0\n",
       "ATL_17_9        51.0\n",
       "ATL_18_13       16.0\n",
       "ATL_18_14       72.0\n",
       "ATL_18_16     1012.0\n",
       "ATL_18_2        11.0\n",
       "ATL_18_3      1021.0\n",
       "ATL_18_6        37.0\n",
       "ATL_18_8        16.0\n",
       "ATL_18_9         7.0\n",
       "ATL_19_10     1009.0\n",
       "ATL_19_13     1008.0\n",
       "ATL_19_18       -3.0\n",
       "ATL_19_19       12.0\n",
       "ATL_19_2        12.0\n",
       "ATL_19_5        34.0\n",
       "ATL_19_9         3.0\n",
       "ATL_20_13     1009.0\n",
       "ATL_20_14       17.0\n",
       "ATL_20_16        6.0\n",
       "ATL_20_17     1009.0\n",
       "ATL_20_19        1.0\n",
       "ATL_20_20       11.0\n",
       "ATL_20_26     1008.0\n",
       "ATL_20_27       13.0\n",
       "ATL_20_28     1009.0\n",
       "ATL_20_29     1008.0\n",
       "ATL_20_31        0.0\n",
       "ATL_20_8        27.0\n",
       "ATL_20_9         9.0\n",
       "CPAC_13_1        0.0\n",
       "CPAC_14_2        9.0\n",
       "CPAC_15_1       29.0\n",
       "CPAC_15_4        7.0\n",
       "CPAC_15_7        1.0\n",
       "CPAC_16_1       21.0\n",
       "CPAC_18_1       35.0\n",
       "EPAC_10_4       10.0\n",
       "EPAC_10_9        0.0\n",
       "EPAC_11_1       44.0\n",
       "EPAC_11_10      21.0\n",
       "EPAC_11_11      20.0\n",
       "EPAC_11_13      39.0\n",
       "EPAC_11_2        0.0\n",
       "EPAC_11_3       17.0\n",
       "EPAC_11_4       55.0\n",
       "EPAC_11_5       -2.0\n",
       "EPAC_11_7       -1.0\n",
       "EPAC_11_9       19.0\n",
       "EPAC_12_12       0.0\n",
       "EPAC_12_16      20.0\n",
       "EPAC_12_3       26.0\n",
       "EPAC_12_4        0.0\n",
       "EPAC_12_6       -1.0\n",
       "EPAC_12_7        0.0\n",
       "EPAC_12_9        0.0\n",
       "EPAC_13_13      18.0\n",
       "EPAC_13_17      38.0\n",
       "EPAC_13_2        9.0\n",
       "EPAC_13_3        0.0\n",
       "EPAC_13_4       13.0\n",
       "EPAC_13_5       14.0\n",
       "EPAC_13_7       14.0\n",
       "EPAC_13_8       20.0\n",
       "EPAC_14_11      17.0\n",
       "EPAC_14_18      15.0\n",
       "EPAC_14_7        3.0\n",
       "EPAC_14_9       30.0\n",
       "EPAC_15_1       17.0\n",
       "EPAC_15_10    1007.0\n",
       "EPAC_15_12      21.0\n",
       "EPAC_15_13    1008.0\n",
       "EPAC_15_15      -4.0\n",
       "EPAC_15_17      12.0\n",
       "EPAC_15_19      20.0\n",
       "EPAC_15_2     1008.0\n",
       "EPAC_15_20       4.0\n",
       "EPAC_15_22      31.0\n",
       "EPAC_15_3     1008.0\n",
       "EPAC_15_5       -2.0\n",
       "EPAC_15_9       22.0\n",
       "EPAC_16_13      23.0\n",
       "EPAC_16_14      30.0\n",
       "EPAC_16_15    1009.0\n",
       "EPAC_16_16       0.0\n",
       "EPAC_16_17    1008.0\n",
       "EPAC_16_19    1009.0\n",
       "EPAC_16_20      38.0\n",
       "EPAC_16_22    1008.0\n",
       "EPAC_16_3        5.0\n",
       "EPAC_16_4        0.0\n",
       "EPAC_16_5     1010.0\n",
       "EPAC_16_7       10.0\n",
       "EPAC_16_8       33.0\n",
       "EPAC_17_16    1008.0\n",
       "EPAC_17_9        0.0\n",
       "EPAC_18_10    1010.0\n",
       "EPAC_18_12    1009.0\n",
       "EPAC_18_14    1009.0\n",
       "EPAC_18_15       3.0\n",
       "EPAC_18_16    1009.0\n",
       "EPAC_18_17    1009.0\n",
       "EPAC_18_2       43.0\n",
       "EPAC_18_20    1009.0\n",
       "EPAC_18_21      21.0\n",
       "EPAC_18_24    1007.0\n",
       "EPAC_18_3        0.0\n",
       "EPAC_18_7     1009.0\n",
       "EPAC_19_1        0.0\n",
       "EPAC_19_11      -7.0\n",
       "EPAC_19_13      28.0\n",
       "EPAC_19_15    1009.0\n",
       "EPAC_19_2     1008.0\n",
       "EPAC_19_6     1009.0\n",
       "EPAC_19_7        9.0\n",
       "EPAC_20_12       1.0\n",
       "EPAC_20_18      29.0\n",
       "EPAC_20_8       18.0\n",
       "EPAC_20_9       -2.0\n",
       "IO_10_1         18.0\n",
       "IO_10_3         38.0\n",
       "IO_10_4        -30.0\n",
       "IO_10_5         30.0\n",
       "IO_11_6         30.0\n",
       "IO_12_6         30.0\n",
       "IO_13_6          4.0\n",
       "IO_14_3          3.0\n",
       "IO_14_4         37.0\n",
       "IO_15_4         15.0\n",
       "IO_15_5         44.0\n",
       "IO_16_5         33.0\n",
       "IO_17_3         18.0\n",
       "IO_18_2         -6.0\n",
       "IO_18_5          0.0\n",
       "IO_18_6         44.0\n",
       "IO_18_7         28.0\n",
       "IO_19_1         65.0\n",
       "IO_19_2         19.0\n",
       "IO_19_3          1.0\n",
       "IO_19_4         43.0\n",
       "IO_19_5         28.0\n",
       "IO_20_1         44.0\n",
       "IO_20_2         -5.0\n",
       "IO_20_3          NaN\n",
       "IO_20_4          5.0\n",
       "SHEM_10_1       30.0\n",
       "SHEM_10_12       4.0\n",
       "SHEM_10_14      30.0\n",
       "SHEM_10_15      26.0\n",
       "SHEM_10_16      41.0\n",
       "SHEM_10_19      11.0\n",
       "SHEM_10_20      19.0\n",
       "SHEM_10_21       1.0\n",
       "SHEM_10_22      19.0\n",
       "SHEM_10_23      18.0\n",
       "SHEM_10_3       30.0\n",
       "SHEM_10_6       37.0\n",
       "SHEM_10_7       34.0\n",
       "SHEM_10_8       15.0\n",
       "SHEM_11_10      26.0\n",
       "SHEM_11_11      71.0\n",
       "SHEM_11_15       4.0\n",
       "SHEM_11_16       0.0\n",
       "SHEM_11_17      23.0\n",
       "SHEM_11_19       8.0\n",
       "SHEM_11_7       -7.0\n",
       "SHEM_11_8        0.0\n",
       "SHEM_12_1        7.0\n",
       "SHEM_12_10      29.0\n",
       "SHEM_12_12      67.0\n",
       "SHEM_12_16      23.0\n",
       "SHEM_12_17      22.0\n",
       "SHEM_12_4        0.0\n",
       "SHEM_12_7       15.0\n",
       "SHEM_12_8        8.0\n",
       "SHEM_12_9        4.0\n",
       "SHEM_13_10      23.0\n",
       "SHEM_13_13      26.0\n",
       "SHEM_13_15      19.0\n",
       "SHEM_13_16      26.0\n",
       "SHEM_13_17      -4.0\n",
       "SHEM_13_19       2.0\n",
       "SHEM_13_21      26.0\n",
       "SHEM_13_22       7.0\n",
       "SHEM_13_23      22.0\n",
       "SHEM_13_3       -4.0\n",
       "SHEM_13_7        0.0\n",
       "SHEM_13_8        1.0\n",
       "SHEM_14_15      -3.0\n",
       "SHEM_14_17      45.0\n",
       "SHEM_14_18       0.0\n",
       "SHEM_14_21      60.0\n",
       "SHEM_14_23      56.0\n",
       "SHEM_14_24      23.0\n",
       "SHEM_14_7        0.0\n",
       "SHEM_14_8       37.0\n",
       "SHEM_15_1       22.0\n",
       "SHEM_15_10      22.0\n",
       "SHEM_15_12      52.0\n",
       "SHEM_15_13      62.0\n",
       "SHEM_15_17      48.0\n",
       "SHEM_15_18      37.0\n",
       "SHEM_15_19     -11.0\n",
       "SHEM_15_21      41.0\n",
       "SHEM_15_22      11.0\n",
       "SHEM_15_24      33.0\n",
       "SHEM_15_4       18.0\n",
       "SHEM_15_9       -2.0\n",
       "SHEM_16_11      27.0\n",
       "SHEM_16_13      41.0\n",
       "SHEM_16_15      41.0\n",
       "SHEM_16_18      33.0\n",
       "SHEM_16_19      38.0\n",
       "SHEM_16_20      43.0\n",
       "SHEM_16_6       30.0\n",
       "SHEM_16_7       -8.0\n",
       "SHEM_16_8       18.0\n",
       "SHEM_16_9       19.0\n",
       "SHEM_17_13      49.0\n",
       "SHEM_17_15      39.0\n",
       "SHEM_17_16      32.0\n",
       "SHEM_17_17      23.0\n",
       "SHEM_17_18      37.0\n",
       "SHEM_17_19      21.0\n",
       "SHEM_17_4       18.0\n",
       "SHEM_17_5       -4.0\n",
       "SHEM_17_9        4.0\n",
       "SHEM_18_10      19.0\n",
       "SHEM_18_11      -6.0\n",
       "SHEM_18_12      20.0\n",
       "SHEM_18_15      12.0\n",
       "SHEM_18_16      31.0\n",
       "SHEM_18_19       0.0\n",
       "SHEM_18_20      -6.0\n",
       "SHEM_18_21       0.0\n",
       "SHEM_18_3       28.0\n",
       "SHEM_18_4       16.0\n",
       "SHEM_18_6       20.0\n",
       "SHEM_18_7       25.0\n",
       "SHEM_18_9       14.0\n",
       "SHEM_19_11      26.0\n",
       "SHEM_19_12      35.0\n",
       "SHEM_19_13      27.0\n",
       "SHEM_19_15      19.0\n",
       "SHEM_19_16      -4.0\n",
       "SHEM_19_17      20.0\n",
       "SHEM_19_18      33.0\n",
       "SHEM_19_19      34.0\n",
       "SHEM_19_20      33.0\n",
       "SHEM_19_21      52.0\n",
       "SHEM_19_22      29.0\n",
       "SHEM_19_23      25.0\n",
       "SHEM_19_24      19.0\n",
       "SHEM_19_25       4.0\n",
       "SHEM_19_3       29.0\n",
       "SHEM_19_5       26.0\n",
       "SHEM_19_6       37.0\n",
       "SHEM_19_7       23.0\n",
       "SHEM_20_1       14.0\n",
       "SHEM_20_14       0.0\n",
       "SHEM_20_15      -3.0\n",
       "SHEM_20_16      21.0\n",
       "SHEM_20_2       19.0\n",
       "SHEM_20_20      29.0\n",
       "SHEM_20_22      32.0\n",
       "SHEM_20_24      -7.0\n",
       "SHEM_20_25      32.0\n",
       "SHEM_20_3       15.0\n",
       "SHEM_20_4       11.0\n",
       "SHEM_20_5        1.0\n",
       "SHEM_20_7       22.0\n",
       "SHEM_20_8        2.0\n",
       "WPAC_10_11      33.0\n",
       "WPAC_10_12      30.0\n",
       "WPAC_10_13      18.0\n",
       "WPAC_10_15      49.0\n",
       "WPAC_10_16      22.0\n",
       "WPAC_10_3       19.0\n",
       "WPAC_10_4       11.0\n",
       "WPAC_11_10      14.0\n",
       "WPAC_11_11      15.0\n",
       "WPAC_11_12       0.0\n",
       "WPAC_11_14      26.0\n",
       "WPAC_11_18       4.0\n",
       "WPAC_11_20      23.0\n",
       "WPAC_11_22      48.0\n",
       "WPAC_11_4       19.0\n",
       "WPAC_11_8        4.0\n",
       "WPAC_12_10      34.0\n",
       "WPAC_12_11       3.0\n",
       "WPAC_12_12      19.0\n",
       "WPAC_12_14      11.0\n",
       "WPAC_12_15      26.0\n",
       "WPAC_12_16      15.0\n",
       "WPAC_12_17      37.0\n",
       "WPAC_12_18      26.0\n",
       "WPAC_12_2       24.0\n",
       "WPAC_12_22      -7.0\n",
       "WPAC_12_24      23.0\n",
       "WPAC_12_26      60.0\n",
       "WPAC_12_3        0.0\n",
       "WPAC_12_4       -2.0\n",
       "WPAC_12_5        3.0\n",
       "WPAC_12_9       11.0\n",
       "WPAC_13_11      33.0\n",
       "WPAC_13_12      37.0\n",
       "WPAC_13_17      15.0\n",
       "WPAC_13_19      33.0\n",
       "WPAC_13_20      11.0\n",
       "WPAC_13_22       1.0\n",
       "WPAC_13_23      30.0\n",
       "WPAC_13_24      15.0\n",
       "WPAC_13_25      -4.0\n",
       "WPAC_13_26      11.0\n",
       "WPAC_13_28       8.0\n",
       "WPAC_13_29      41.0\n",
       "WPAC_13_31      34.0\n",
       "WPAC_13_6       19.0\n",
       "WPAC_13_7       23.0\n",
       "WPAC_14_10      37.0\n",
       "WPAC_14_11      26.0\n",
       "WPAC_14_15       8.0\n",
       "WPAC_14_18       4.0\n",
       "WPAC_14_19      33.0\n",
       "WPAC_14_20      34.0\n",
       "WPAC_14_22      26.0\n",
       "WPAC_14_3        5.0\n",
       "WPAC_14_6       12.0\n",
       "WPAC_14_8       19.0\n",
       "WPAC_14_9       49.0\n",
       "WPAC_15_1        2.0\n",
       "WPAC_15_10      11.0\n",
       "WPAC_15_11      27.0\n",
       "WPAC_15_13      30.0\n",
       "WPAC_15_16      22.0\n",
       "WPAC_15_17      22.0\n",
       "WPAC_15_2       -2.0\n",
       "WPAC_15_20      23.0\n",
       "WPAC_15_21      41.0\n",
       "WPAC_15_22       0.0\n",
       "WPAC_15_23      -3.0\n",
       "WPAC_15_24      22.0\n",
       "WPAC_15_25      22.0\n",
       "WPAC_15_27      26.0\n",
       "WPAC_15_28      41.0\n",
       "WPAC_15_4       22.0\n",
       "WPAC_15_6       41.0\n",
       "WPAC_15_9       15.0\n",
       "WPAC_16_12      33.0\n",
       "WPAC_16_15      26.0\n",
       "WPAC_16_16      75.0\n",
       "WPAC_16_18      40.0\n",
       "WPAC_16_2       52.0\n",
       "WPAC_16_20      -3.0\n",
       "WPAC_16_21      41.0\n",
       "WPAC_16_23      45.0\n",
       "WPAC_16_24      29.0\n",
       "WPAC_16_25      41.0\n",
       "WPAC_16_26      18.0\n",
       "WPAC_16_30      78.0\n",
       "WPAC_16_6       30.0\n",
       "WPAC_17_11      -1.0\n",
       "WPAC_17_14      24.0\n",
       "WPAC_17_15      22.0\n",
       "WPAC_17_17      -4.0\n",
       "WPAC_17_20      32.0\n",
       "WPAC_17_21      11.0\n",
       "WPAC_17_24       1.0\n",
       "WPAC_17_25      10.0\n",
       "WPAC_17_27       9.0\n",
       "WPAC_17_28       0.0\n",
       "WPAC_17_33       4.0\n",
       "WPAC_17_7       36.0\n",
       "WPAC_18_10      49.0\n",
       "WPAC_18_14       0.0\n",
       "WPAC_18_15      35.0\n",
       "WPAC_18_17      18.0\n",
       "WPAC_18_19      15.0\n",
       "WPAC_18_22      11.0\n",
       "WPAC_18_23      18.0\n",
       "WPAC_18_25      22.0\n",
       "WPAC_18_26      50.0\n",
       "WPAC_18_28      33.0\n",
       "WPAC_18_3       51.0\n",
       "WPAC_18_30      37.0\n",
       "WPAC_18_31      31.0\n",
       "WPAC_18_33      35.0\n",
       "WPAC_18_34      44.0\n",
       "WPAC_18_9       11.0\n",
       "WPAC_19_10      13.0\n",
       "WPAC_19_11      18.0\n",
       "WPAC_19_14      40.0\n",
       "WPAC_19_15      11.0\n",
       "WPAC_19_19      22.0\n",
       "WPAC_19_2       45.0\n",
       "WPAC_19_20      34.0\n",
       "WPAC_19_21      24.0\n",
       "WPAC_19_22      21.0\n",
       "WPAC_19_23      27.0\n",
       "WPAC_19_24      49.0\n",
       "WPAC_19_25       0.0\n",
       "WPAC_19_26      50.0\n",
       "WPAC_19_27      23.0\n",
       "WPAC_19_28      15.0\n",
       "WPAC_19_29      32.0\n",
       "WPAC_19_9       22.0\n",
       "WPAC_20_1       28.0\n",
       "WPAC_20_10      30.0\n",
       "WPAC_20_11      38.0\n",
       "WPAC_20_15      -5.0\n",
       "WPAC_20_16      22.0\n",
       "WPAC_20_19      24.0\n",
       "WPAC_20_21      36.0\n",
       "WPAC_20_22     108.0\n",
       "WPAC_20_25      17.0\n",
       "WPAC_20_3       27.0\n",
       "WPAC_20_7       -2.0\n",
       "WPAC_20_9       -6.0\n",
       "Name: 24_hour_pressure_diff, dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Sort by 'storm_code' and 'time_bin'\n",
    "rainband_timebin_joined = rainband_timebin_joined.sort_values(by=['storm_code', 'time_bin'])\n",
    "\n",
    "# Calculate the 24-hour difference for the 'knots' column (using 'shift' to get the previous 24-hour value)\n",
    "rainband_timebin_joined['24_hour_knots_diff'] = rainband_timebin_joined['knots'] - rainband_timebin_joined.groupby('storm_code')['knots'].shift(periods=48)\n",
    "rainband_timebin_joined['24_hour_pressure_diff'] = rainband_timebin_joined['pressure'] - rainband_timebin_joined.groupby('storm_code')['pressure'].shift(periods=48)\n",
    "\n",
    "# Save or display the DataFrame\n",
    "rainband_timebin_joined.groupby('storm_code')['24_hour_knots_diff'].max()\n",
    "rainband_timebin_joined.groupby('storm_code')['24_hour_pressure_diff'].max()\n",
    "\n",
    "rainband_timebin_joined.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "innercore_timebin_joined['TC_Category'] = innercore_timebin_joined.apply(knot_category, axis=1)\n",
    "innercore_timebin_joined['Intensification_Category'] = innercore_timebin_joined.apply(intensification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TC_Category   TC_Category \n",
       "1             1                27327\n",
       "2             2                13597\n",
       "3             3                10667\n",
       "4             4                10300\n",
       "5             5                 2530\n",
       "Unidentified  Unidentified    122699\n",
       "Name: TC_Category, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined.groupby('TC_Category')['TC_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intensification_Category  Intensification_Category\n",
       "Intensifying              Intensifying                54830\n",
       "Neutral                   Neutral                     66873\n",
       "Unidentified              Unidentified                46053\n",
       "Weakning                  Weakning                    19364\n",
       "Name: Intensification_Category, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innercore_timebin_joined.groupby('Intensification_Category')['Intensification_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate current category based on current wind speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122699"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((innercore_timebin_joined['knots'] < 64) == True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
